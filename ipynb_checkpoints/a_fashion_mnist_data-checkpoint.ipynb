{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad2f7131",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: wandb in /home/work/.local/lib/python3.10/site-packages (0.18.7)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/work/.local/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.26)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.4)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/work/.local/lib/python3.10/site-packages (from wandb) (2.19.0)\n",
      "Requirement already satisfied: setproctitle in /home/work/.local/lib/python3.10/site-packages (from wandb) (1.3.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (68.2.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.10.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
      "Requirement already satisfied: smmap<4,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (3.0.5)\n",
      "\u001b[33mDEPRECATION: devscripts 2.22.1ubuntu1 has a non-standard version number. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of devscripts or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "577980fd",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: wandb\n",
      "Version: 0.18.7\n",
      "Summary: A CLI and library for interacting with the Weights & Biases API.\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: Weights & Biases <support@wandb.com>\n",
      "License: MIT License\n",
      "        \n",
      "        Copyright (c) 2021 Weights and Biases, Inc.\n",
      "        \n",
      "        Permission is hereby granted, free of charge, to any person obtaining a copy\n",
      "        of this software and associated documentation files (the \"Software\"), to deal\n",
      "        in the Software without restriction, including without limitation the rights\n",
      "        to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
      "        copies of the Software, and to permit persons to whom the Software is\n",
      "        furnished to do so, subject to the following conditions:\n",
      "        \n",
      "        The above copyright notice and this permission notice shall be included in all\n",
      "        copies or substantial portions of the Software.\n",
      "        \n",
      "        THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
      "        IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
      "        FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
      "        AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
      "        LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
      "        OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
      "        SOFTWARE.\n",
      "Location: /home/work/.local/lib/python3.10/site-packages\n",
      "Requires: click, docker-pycreds, gitpython, platformdirs, protobuf, psutil, pyyaml, requests, sentry-sdk, setproctitle, setuptools, typing-extensions\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b79767cb",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: wandb in /home/work/.local/lib/python3.10/site-packages (0.18.7)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/work/.local/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.26)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.4)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/work/.local/lib/python3.10/site-packages (from wandb) (2.19.0)\n",
      "Requirement already satisfied: setproctitle in /home/work/.local/lib/python3.10/site-packages (from wandb) (1.3.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (68.2.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.10.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
      "Requirement already satisfied: smmap<4,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (3.0.5)\n",
      "\u001b[33mDEPRECATION: devscripts 2.22.1ubuntu1 has a non-standard version number. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of devscripts or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57c0e7b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function wandb.sdk.wandb_login.login(anonymous: Optional[Literal['must', 'allow', 'never']] = None, key: Optional[str] = None, relogin: Optional[bool] = None, host: Optional[str] = None, force: Optional[bool] = None, timeout: Optional[int] = None, verify: bool = False) -> bool>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f050ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "420f2f78",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "please, restart the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ffda20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "print(wandb.api.viewer())  # 로그인된 사용자 정보 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab38e96d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/work/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login(key=\"afebcaccd9929fcb34d6e10db06a3c432acfc56e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb4fd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FashionMNIST의 평균과 표준편차 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c8f15c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated Mean: 0.28604060411453247\n",
      "Calculated Std: 0.3530242443084717\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# FashionMNIST 데이터셋 로드\n",
    "mnist_train = datasets.FashionMNIST(root='data', train=True, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# 모든 데이터를 하나의 텐서로 합치기\n",
    "all_images = torch.cat([img[0].view(-1) for img, _ in mnist_train])\n",
    "\n",
    "# 평균과 표준편차 계산\n",
    "mean = all_images.mean().item()\n",
    "std = all_images.std().item()\n",
    "\n",
    "print(f\"Calculated Mean: {mean}\")\n",
    "print(f\"Calculated Std: {std}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "initial_id",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/work/DL\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'wandb' has no attribute 'init'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 72\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     71\u001b[0m     config \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2048\u001b[39m, }\n\u001b[0;32m---> 72\u001b[0m     \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisabled\u001b[39m\u001b[38;5;124m\"\u001b[39m, config\u001b[38;5;241m=\u001b[39mconfig)\n\u001b[1;32m     74\u001b[0m     train_data_loader, validation_data_loader, f_mnist_transforms \u001b[38;5;241m=\u001b[39m get_fashion_mnist_data()\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28mprint\u001b[39m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'wandb' has no attribute 'init'"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "# Use the current working directory\n",
    "BASE_PATH = Path().resolve()\n",
    "print(BASE_PATH)\n",
    "\n",
    "import sys\n",
    "sys.path.append(str(BASE_PATH))\n",
    "\n",
    "from utils import get_num_cpu_cores, is_linux, is_windows\n",
    "\n",
    "\n",
    "def get_fashion_mnist_data():\n",
    "    data_path = os.path.join(BASE_PATH, \"_00_data\", \"j_fashion_mnist\")\n",
    "\n",
    "    f_mnist_train = datasets.FashionMNIST(data_path, train=True, download=True, transform=transforms.ToTensor())\n",
    "    f_mnist_train, f_mnist_validation = random_split(f_mnist_train, [55_000, 5_000])\n",
    "\n",
    "    print(\"Num Train Samples: \", len(f_mnist_train))\n",
    "    print(\"Num Validation Samples: \", len(f_mnist_validation))\n",
    "    print(\"Sample Shape: \", f_mnist_train[0][0].shape)  # torch.Size([1, 28, 28])\n",
    "\n",
    "    num_data_loading_workers = get_num_cpu_cores() if is_linux() or is_windows() else 0\n",
    "    print(\"Number of Data Loading Workers:\", num_data_loading_workers)\n",
    "\n",
    "    train_data_loader = DataLoader(\n",
    "        dataset=f_mnist_train, batch_size=wandb.config.batch_size, shuffle=True,\n",
    "        pin_memory=True, num_workers=num_data_loading_workers\n",
    "    )\n",
    "\n",
    "    validation_data_loader = DataLoader(\n",
    "        dataset=f_mnist_validation, batch_size=wandb.config.batch_size,\n",
    "        pin_memory=True, num_workers=num_data_loading_workers\n",
    "    )\n",
    "\n",
    "    f_mnist_transforms = nn.Sequential(\n",
    "        transforms.ConvertImageDtype(torch.float),\n",
    "        transforms.Normalize(mean=0.28604060411453247, std=0.3530242443084717),\n",
    "    )\n",
    "\n",
    "    return train_data_loader, validation_data_loader, f_mnist_transforms\n",
    "\n",
    "\n",
    "def get_fashion_mnist_test_data():\n",
    "    data_path = os.path.join(BASE_PATH, \"_00_data\", \"j_fashion_mnist\")\n",
    "\n",
    "    f_mnist_test_images = datasets.FashionMNIST(data_path, train=False, download=True)\n",
    "    f_mnist_test = datasets.FashionMNIST(data_path, train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "    print(\"Num Test Samples: \", len(f_mnist_test))\n",
    "    print(\"Sample Shape: \", f_mnist_test[0][0].shape)  # torch.Size([1, 28, 28])\n",
    "\n",
    "    test_data_loader = DataLoader(dataset=f_mnist_test, batch_size=len(f_mnist_test))\n",
    "\n",
    "    f_mnist_transforms = nn.Sequential(\n",
    "        transforms.ConvertImageDtype(torch.float),\n",
    "        transforms.Normalize(mean=0.28604060411453247, std=0.3530242443084717),\n",
    "    )    \n",
    "\n",
    "    return f_mnist_test_images, test_data_loader, f_mnist_transforms\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = {'batch_size': 2048, }\n",
    "    wandb.init(mode=\"disabled\", config=config)\n",
    "\n",
    "    train_data_loader, validation_data_loader, f_mnist_transforms = get_fashion_mnist_data()\n",
    "    print()\n",
    "    f_mnist_test_images, test_data_loader, f_mnist_transforms = get_fashion_mnist_test_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0313f74",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.18.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.26)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.4)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-2.18.0-py2.py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting setproctitle (from wandb)\n",
      "  Downloading setproctitle-1.3.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (68.2.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.10.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
      "Requirement already satisfied: smmap<4,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (3.0.5)\n",
      "Downloading wandb-0.18.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.1/16.1 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Downloading sentry_sdk-2.18.0-py2.py3-none-any.whl (317 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.5/317.5 kB\u001b[0m \u001b[31m263.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading setproctitle-1.3.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "\u001b[33mDEPRECATION: devscripts 2.22.1ubuntu1 has a non-standard version number. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of devscripts or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: setproctitle, sentry-sdk, docker-pycreds, wandb\n",
      "Successfully installed docker-pycreds-0.4.0 sentry-sdk-2.18.0 setproctitle-1.3.4 wandb-0.18.7\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9eb3384d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/work/DL/wandb/run-20241121_021819-w1ih6a4i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/test/runs/w1ih6a4i' target=\"_blank\">cosmic-silence-11</a></strong> to <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/test' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/test/runs/w1ih6a4i' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/test/runs/w1ih6a4i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁▃▅▆▆▇██</td></tr><tr><td>loss</td><td>█▃▄▄▂▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.97855</td></tr><tr><td>loss</td><td>0.02925</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cosmic-silence-11</strong> at: <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/test/runs/w1ih6a4i' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/test/runs/w1ih6a4i</a><br/> View project at: <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/test' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/test</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241121_021819-w1ih6a4i/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "import random\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"test\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"architecture\": \"CNN\",\n",
    "    \"dataset\": \"FashionMNIST\",\n",
    "    \"epochs\": 10,\n",
    "    }\n",
    ")\n",
    "\n",
    "# simulate training\n",
    "epochs = 10\n",
    "offset = random.random() / 5\n",
    "for epoch in range(2, epochs):\n",
    "    acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n",
    "    loss = 2 ** -epoch + random.random() / epoch + offset\n",
    "\n",
    "    # log metrics to wandb\n",
    "    wandb.log({\"acc\": acc, \"loss\": loss})\n",
    "\n",
    "# [optional] finish the wandb run, necessary in notebooks\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a9ffff74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/work/DL\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/work/DL/wandb/run-20241121_044938-v288wwlz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/test/runs/v288wwlz' target=\"_blank\">stilted-frog-12</a></strong> to <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/test' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/test/runs/v288wwlz' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/test/runs/v288wwlz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Train Samples:  55000\n",
      "Num Validation Samples:  5000\n",
      "Sample Shape:  torch.Size([1, 28, 28])\n",
      "Number of Data Loading Workers: 2\n",
      "Epoch [1/10], Train Loss: 0.6043, Train Accuracy: 0.7959, Validation Loss: 0.4444, Validation Accuracy: 0.8384\n",
      "Epoch [2/10], Train Loss: 0.4274, Train Accuracy: 0.8504, Validation Loss: 0.3855, Validation Accuracy: 0.8618\n",
      "Epoch [3/10], Train Loss: 0.3856, Train Accuracy: 0.8631, Validation Loss: 0.3590, Validation Accuracy: 0.8672\n",
      "Epoch [4/10], Train Loss: 0.3579, Train Accuracy: 0.8731, Validation Loss: 0.3509, Validation Accuracy: 0.8704\n",
      "Epoch [5/10], Train Loss: 0.3362, Train Accuracy: 0.8803, Validation Loss: 0.3203, Validation Accuracy: 0.8814\n",
      "Epoch [6/10], Train Loss: 0.3199, Train Accuracy: 0.8845, Validation Loss: 0.3134, Validation Accuracy: 0.8848\n",
      "Epoch [7/10], Train Loss: 0.3060, Train Accuracy: 0.8901, Validation Loss: 0.3122, Validation Accuracy: 0.8844\n",
      "Epoch [8/10], Train Loss: 0.2927, Train Accuracy: 0.8948, Validation Loss: 0.3073, Validation Accuracy: 0.8880\n",
      "Epoch [9/10], Train Loss: 0.2838, Train Accuracy: 0.8972, Validation Loss: 0.3079, Validation Accuracy: 0.8860\n",
      "Epoch [10/10], Train Loss: 0.2759, Train Accuracy: 0.8987, Validation Loss: 0.3127, Validation Accuracy: 0.8838\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▅▆▆▇▇▇███</td></tr><tr><td>train_loss</td><td>█▄▃▃▂▂▂▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▅▆▇█▇██▇</td></tr><tr><td>val_loss</td><td>█▅▄▃▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train_accuracy</td><td>0.89871</td></tr><tr><td>train_loss</td><td>0.27593</td></tr><tr><td>val_accuracy</td><td>0.8838</td></tr><tr><td>val_loss</td><td>0.31267</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">stilted-frog-12</strong> at: <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/test/runs/v288wwlz' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/test/runs/v288wwlz</a><br/> View project at: <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/test' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/test</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241121_044938-v288wwlz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 기존 코드에서 Training loss, validation loss, Training Accuracy, Validation Accuracy 넣기.version 1\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "# Use the current working directory\n",
    "BASE_PATH = Path().resolve()\n",
    "print(BASE_PATH)\n",
    "\n",
    "import sys\n",
    "sys.path.append(str(BASE_PATH))\n",
    "\n",
    "from utils import get_num_cpu_cores, is_linux, is_windows\n",
    "\n",
    "def get_fashion_mnist_data():\n",
    "    data_path = os.path.join(BASE_PATH, \"_00_data\", \"j_fashion_mnist\")\n",
    "\n",
    "    f_mnist_train = datasets.FashionMNIST(data_path, train=True, download=True, transform=transforms.ToTensor())\n",
    "    f_mnist_train, f_mnist_validation = random_split(f_mnist_train, [55_000, 5_000])\n",
    "\n",
    "    print(\"Num Train Samples: \", len(f_mnist_train))\n",
    "    print(\"Num Validation Samples: \", len(f_mnist_validation))\n",
    "    print(\"Sample Shape: \", f_mnist_train[0][0].shape)  # torch.Size([1, 28, 28])\n",
    "\n",
    "    num_data_loading_workers = get_num_cpu_cores() if is_linux() or is_windows() else 0\n",
    "    print(\"Number of Data Loading Workers:\", num_data_loading_workers)\n",
    "\n",
    "    train_data_loader = DataLoader(\n",
    "        dataset=f_mnist_train, batch_size=wandb.config.batch_size, shuffle=True,\n",
    "        pin_memory=True, num_workers=num_data_loading_workers\n",
    "    )\n",
    "\n",
    "    validation_data_loader = DataLoader(\n",
    "        dataset=f_mnist_validation, batch_size=wandb.config.batch_size,\n",
    "        pin_memory=True, num_workers=num_data_loading_workers\n",
    "    )\n",
    "\n",
    "    return train_data_loader, validation_data_loader\n",
    "\n",
    "def train_model(train_data_loader, validation_data_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Define model\n",
    "    model = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(28 * 28, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 10)\n",
    "    ).to(device)\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=wandb.config.learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Training and Validation loop\n",
    "    for epoch in range(1, wandb.config.epochs + 1):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in train_data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_accuracy = correct / total\n",
    "        train_loss /= len(train_data_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in validation_data_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_accuracy = val_correct / val_total\n",
    "        val_loss /= len(validation_data_loader)\n",
    "\n",
    "        # Log metrics to wandb\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_accuracy\": train_accuracy,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_accuracy\": val_accuracy\n",
    "        })\n",
    "\n",
    "        print(f\"Epoch [{epoch}/{wandb.config.epochs}], Train Loss: {train_loss:.4f}, \"\n",
    "              f\"Train Accuracy: {train_accuracy:.4f}, Validation Loss: {val_loss:.4f}, \"\n",
    "              f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize WandB\n",
    "    wandb.init(\n",
    "        project=\"test\",\n",
    "        config={\n",
    "            \"learning_rate\": 0.001,\n",
    "            \"batch_size\": 128,\n",
    "            \"epochs\": 10,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Get Data Loaders\n",
    "    train_data_loader, validation_data_loader = get_fashion_mnist_data()\n",
    "\n",
    "    # Train Model\n",
    "    train_model(train_data_loader, validation_data_loader)\n",
    "\n",
    "    # Finish WandB run\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "25f8fb18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/work/DL\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/work/DL/wandb/run-20241121_045349-8njt7ez6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/test/runs/8njt7ez6' target=\"_blank\">spring-morning-13</a></strong> to <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/test' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/test/runs/8njt7ez6' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/test/runs/8njt7ez6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Train Samples:  55000\n",
      "Num Validation Samples:  5000\n",
      "Sample Shape:  torch.Size([1, 28, 28])\n",
      "Number of Data Loading Workers: 2\n",
      "Epoch [1/10], Train Loss: 0.6650, Train Accuracy: 0.7707, Validation Loss: 0.4601, Validation Accuracy: 0.8384\n",
      "Epoch [2/10], Train Loss: 0.4762, Train Accuracy: 0.8316, Validation Loss: 0.4294, Validation Accuracy: 0.8400\n",
      "Epoch [3/10], Train Loss: 0.4354, Train Accuracy: 0.8436, Validation Loss: 0.3739, Validation Accuracy: 0.8646\n",
      "Epoch [4/10], Train Loss: 0.4115, Train Accuracy: 0.8512, Validation Loss: 0.3618, Validation Accuracy: 0.8684\n",
      "Epoch [5/10], Train Loss: 0.3939, Train Accuracy: 0.8557, Validation Loss: 0.3453, Validation Accuracy: 0.8710\n",
      "Epoch [6/10], Train Loss: 0.3773, Train Accuracy: 0.8613, Validation Loss: 0.3434, Validation Accuracy: 0.8700\n",
      "Epoch [7/10], Train Loss: 0.3667, Train Accuracy: 0.8653, Validation Loss: 0.3268, Validation Accuracy: 0.8812\n",
      "Epoch [8/10], Train Loss: 0.3591, Train Accuracy: 0.8683, Validation Loss: 0.3154, Validation Accuracy: 0.8816\n",
      "Epoch [9/10], Train Loss: 0.3527, Train Accuracy: 0.8708, Validation Loss: 0.3290, Validation Accuracy: 0.8764\n",
      "Epoch [10/10], Train Loss: 0.3479, Train Accuracy: 0.8718, Validation Loss: 0.3213, Validation Accuracy: 0.8806\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▅▆▇▇▇████</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▁▅▆▆▆██▇█</td></tr><tr><td>val_loss</td><td>█▇▄▃▂▂▂▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train_accuracy</td><td>0.87178</td></tr><tr><td>train_loss</td><td>0.34789</td></tr><tr><td>val_accuracy</td><td>0.8806</td></tr><tr><td>val_loss</td><td>0.32127</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">spring-morning-13</strong> at: <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/test/runs/8njt7ez6' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/test/runs/8njt7ez6</a><br/> View project at: <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/test' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/test</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241121_045349-8njt7ez6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 기존 코드에서 Training loss, validation loss, Training Accuracy, Validation Accuracy 넣기. version 1 dropout()추가\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "# Use the current working directory\n",
    "BASE_PATH = Path().resolve()\n",
    "print(BASE_PATH)\n",
    "\n",
    "import sys\n",
    "sys.path.append(str(BASE_PATH))\n",
    "\n",
    "from utils import get_num_cpu_cores, is_linux, is_windows\n",
    "\n",
    "def get_fashion_mnist_data():\n",
    "    data_path = os.path.join(BASE_PATH, \"_00_data\", \"j_fashion_mnist\")\n",
    "\n",
    "    f_mnist_train = datasets.FashionMNIST(data_path, train=True, download=True, transform=transforms.ToTensor())\n",
    "    f_mnist_train, f_mnist_validation = random_split(f_mnist_train, [55_000, 5_000])\n",
    "\n",
    "    print(\"Num Train Samples: \", len(f_mnist_train))\n",
    "    print(\"Num Validation Samples: \", len(f_mnist_validation))\n",
    "    print(\"Sample Shape: \", f_mnist_train[0][0].shape)  # torch.Size([1, 28, 28])\n",
    "\n",
    "    num_data_loading_workers = get_num_cpu_cores() if is_linux() or is_windows() else 0\n",
    "    print(\"Number of Data Loading Workers:\", num_data_loading_workers)\n",
    "\n",
    "    train_data_loader = DataLoader(\n",
    "        dataset=f_mnist_train, batch_size=wandb.config.batch_size, shuffle=True,\n",
    "        pin_memory=True, num_workers=num_data_loading_workers\n",
    "    )\n",
    "\n",
    "    validation_data_loader = DataLoader(\n",
    "        dataset=f_mnist_validation, batch_size=wandb.config.batch_size,\n",
    "        pin_memory=True, num_workers=num_data_loading_workers\n",
    "    )\n",
    "\n",
    "    return train_data_loader, validation_data_loader\n",
    "\n",
    "def train_model(train_data_loader, validation_data_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Define model\n",
    "    model = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Dropout(p=0.2),  # 드롭아웃 확률 20%\n",
    "        nn.Linear(28 * 28, 128),\n",
    "        nn.Dropout(p=0.2),  # 드롭아웃 확률 20%\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 10)\n",
    "    ).to(device)\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=wandb.config.learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Training and Validation loop\n",
    "    for epoch in range(1, wandb.config.epochs + 1):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in train_data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_accuracy = correct / total\n",
    "        train_loss /= len(train_data_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in validation_data_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_accuracy = val_correct / val_total\n",
    "        val_loss /= len(validation_data_loader)\n",
    "\n",
    "        # Log metrics to wandb\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_accuracy\": train_accuracy,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_accuracy\": val_accuracy\n",
    "        })\n",
    "\n",
    "        print(f\"Epoch [{epoch}/{wandb.config.epochs}], Train Loss: {train_loss:.4f}, \"\n",
    "              f\"Train Accuracy: {train_accuracy:.4f}, Validation Loss: {val_loss:.4f}, \"\n",
    "              f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize WandB\n",
    "    wandb.init(\n",
    "        project=\"test\",\n",
    "        config={\n",
    "            \"learning_rate\": 0.001,\n",
    "            \"batch_size\": 128,\n",
    "            \"epochs\": 10,\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "599b3376",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/work/DL\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/work/DL/wandb/run-20241121_045543-jozzmaf6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/test/runs/jozzmaf6' target=\"_blank\">ancient-thunder-14</a></strong> to <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/test' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/test/runs/jozzmaf6' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/test/runs/jozzmaf6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Train Samples:  55000\n",
      "Num Validation Samples:  5000\n",
      "Sample Shape:  torch.Size([1, 28, 28])\n",
      "Number of Data Loading Workers: 2\n",
      "Epoch [1/20], Train Loss: 0.6710, Train Accuracy: 0.7660, Validation Loss: 0.4606, Validation Accuracy: 0.8386\n",
      "Epoch [2/20], Train Loss: 0.4767, Train Accuracy: 0.8311, Validation Loss: 0.3910, Validation Accuracy: 0.8594\n",
      "Epoch [3/20], Train Loss: 0.4345, Train Accuracy: 0.8430, Validation Loss: 0.3648, Validation Accuracy: 0.8690\n",
      "Epoch [4/20], Train Loss: 0.4113, Train Accuracy: 0.8513, Validation Loss: 0.3490, Validation Accuracy: 0.8722\n",
      "Epoch [5/20], Train Loss: 0.3929, Train Accuracy: 0.8569, Validation Loss: 0.3349, Validation Accuracy: 0.8794\n",
      "Epoch [6/20], Train Loss: 0.3817, Train Accuracy: 0.8602, Validation Loss: 0.3391, Validation Accuracy: 0.8760\n",
      "Epoch [7/20], Train Loss: 0.3695, Train Accuracy: 0.8650, Validation Loss: 0.3184, Validation Accuracy: 0.8844\n",
      "Epoch [8/20], Train Loss: 0.3642, Train Accuracy: 0.8652, Validation Loss: 0.3166, Validation Accuracy: 0.8824\n",
      "Epoch [9/20], Train Loss: 0.3515, Train Accuracy: 0.8712, Validation Loss: 0.3129, Validation Accuracy: 0.8868\n",
      "Epoch [10/20], Train Loss: 0.3489, Train Accuracy: 0.8712, Validation Loss: 0.3205, Validation Accuracy: 0.8820\n",
      "Epoch [11/20], Train Loss: 0.3414, Train Accuracy: 0.8746, Validation Loss: 0.3096, Validation Accuracy: 0.8814\n",
      "Epoch [12/20], Train Loss: 0.3391, Train Accuracy: 0.8747, Validation Loss: 0.3041, Validation Accuracy: 0.8844\n",
      "Epoch [13/20], Train Loss: 0.3355, Train Accuracy: 0.8767, Validation Loss: 0.3034, Validation Accuracy: 0.8870\n",
      "Epoch [14/20], Train Loss: 0.3307, Train Accuracy: 0.8770, Validation Loss: 0.2939, Validation Accuracy: 0.8896\n",
      "Epoch [15/20], Train Loss: 0.3257, Train Accuracy: 0.8798, Validation Loss: 0.2942, Validation Accuracy: 0.8862\n",
      "Epoch [16/20], Train Loss: 0.3203, Train Accuracy: 0.8806, Validation Loss: 0.2965, Validation Accuracy: 0.8910\n",
      "Epoch [17/20], Train Loss: 0.3207, Train Accuracy: 0.8805, Validation Loss: 0.2972, Validation Accuracy: 0.8900\n",
      "Epoch [18/20], Train Loss: 0.3135, Train Accuracy: 0.8823, Validation Loss: 0.2954, Validation Accuracy: 0.8900\n",
      "Epoch [19/20], Train Loss: 0.3130, Train Accuracy: 0.8831, Validation Loss: 0.2918, Validation Accuracy: 0.8930\n",
      "Epoch [20/20], Train Loss: 0.3094, Train Accuracy: 0.8855, Validation Loss: 0.2922, Validation Accuracy: 0.8900\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>train_accuracy</td><td>▁▅▆▆▆▇▇▇▇▇▇▇▇███████</td></tr><tr><td>train_loss</td><td>█▄▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▅▅▆▆▇▇▇▇▇▇▇█▇█████</td></tr><tr><td>val_loss</td><td>█▅▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>train_accuracy</td><td>0.88547</td></tr><tr><td>train_loss</td><td>0.30944</td></tr><tr><td>val_accuracy</td><td>0.89</td></tr><tr><td>val_loss</td><td>0.29224</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ancient-thunder-14</strong> at: <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/test/runs/jozzmaf6' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/test/runs/jozzmaf6</a><br/> View project at: <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/test' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/test</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241121_045543-jozzmaf6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 기존 코드에서 Training loss, validation loss, Training Accuracy, Validation Accuracy 넣기. dropout()추가\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "# Use the current working directory\n",
    "BASE_PATH = Path().resolve()\n",
    "print(BASE_PATH)\n",
    "\n",
    "import sys\n",
    "sys.path.append(str(BASE_PATH))\n",
    "\n",
    "from utils import get_num_cpu_cores, is_linux, is_windows\n",
    "\n",
    "def get_fashion_mnist_data():\n",
    "    data_path = os.path.join(BASE_PATH, \"_00_data\", \"j_fashion_mnist\")\n",
    "\n",
    "    f_mnist_train = datasets.FashionMNIST(data_path, train=True, download=True, transform=transforms.ToTensor())\n",
    "    f_mnist_train, f_mnist_validation = random_split(f_mnist_train, [55_000, 5_000])\n",
    "\n",
    "    print(\"Num Train Samples: \", len(f_mnist_train))\n",
    "    print(\"Num Validation Samples: \", len(f_mnist_validation))\n",
    "    print(\"Sample Shape: \", f_mnist_train[0][0].shape)  # torch.Size([1, 28, 28])\n",
    "\n",
    "    num_data_loading_workers = get_num_cpu_cores() if is_linux() or is_windows() else 0\n",
    "    print(\"Number of Data Loading Workers:\", num_data_loading_workers)\n",
    "\n",
    "    train_data_loader = DataLoader(\n",
    "        dataset=f_mnist_train, batch_size=wandb.config.batch_size, shuffle=True,\n",
    "        pin_memory=True, num_workers=num_data_loading_workers\n",
    "    )\n",
    "\n",
    "    validation_data_loader = DataLoader(\n",
    "        dataset=f_mnist_validation, batch_size=wandb.config.batch_size,\n",
    "        pin_memory=True, num_workers=num_data_loading_workers\n",
    "    )\n",
    "\n",
    "    return train_data_loader, validation_data_loader\n",
    "\n",
    "def train_model(train_data_loader, validation_data_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Define model\n",
    "    model = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Dropout(p=0.2),  # 드롭아웃 확률 20%\n",
    "        nn.Linear(28 * 28, 128),\n",
    "        nn.Dropout(p=0.2),  # 드롭아웃 확률 20%\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 10)\n",
    "    ).to(device)\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=wandb.config.learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Training and Validation loop\n",
    "    for epoch in range(1, wandb.config.epochs + 1):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in train_data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_accuracy = correct / total\n",
    "        train_loss /= len(train_data_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in validation_data_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_accuracy = val_correct / val_total\n",
    "        val_loss /= len(validation_data_loader)\n",
    "\n",
    "        # Log metrics to wandb\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_accuracy\": train_accuracy,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_accuracy\": val_accuracy\n",
    "        })\n",
    "\n",
    "        print(f\"Epoch [{epoch}/{wandb.config.epochs}], Train Loss: {train_loss:.4f}, \"\n",
    "              f\"Train Accuracy: {train_accuracy:.4f}, Validation Loss: {val_loss:.4f}, \"\n",
    "              f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize WandB\n",
    "    wandb.init(\n",
    "        project=\"test\",\n",
    "        config={\n",
    "            \"learning_rate\": 0.001,\n",
    "            \"batch_size\": 128,\n",
    "            \"epochs\": 20,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Get Data Loaders\n",
    "    train_data_loader, validation_data_loader = get_fashion_mnist_data()\n",
    "\n",
    "    # Train Model\n",
    "    train_model(train_data_loader, validation_data_loader)\n",
    "\n",
    "    # Finish WandB run\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0803666d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/work/DL\n",
      "Num Train Samples:  55000\n",
      "Num Validation Samples:  5000\n",
      "Sample Shape:  torch.Size([1, 28, 28])\n",
      "Number of Data Loading Workers: 2\n",
      "\n",
      "Num Test Samples:  10000\n",
      "Sample Shape:  torch.Size([1, 28, 28])\n",
      "Epoch [1/10] - Train Loss: 1.3306, Train Acc: 0.6026, Val Loss: 0.8133, Val Acc: 0.7254\n",
      "Epoch [2/10] - Train Loss: 0.6967, Train Acc: 0.7597, Val Loss: 0.6381, Val Acc: 0.7904\n",
      "Epoch [3/10] - Train Loss: 0.5830, Train Acc: 0.8039, Val Loss: 0.5646, Val Acc: 0.8132\n",
      "Epoch [4/10] - Train Loss: 0.5284, Train Acc: 0.8230, Val Loss: 0.5252, Val Acc: 0.8300\n",
      "Epoch [5/10] - Train Loss: 0.4954, Train Acc: 0.8314, Val Loss: 0.4999, Val Acc: 0.8384\n",
      "Epoch [6/10] - Train Loss: 0.4713, Train Acc: 0.8390, Val Loss: 0.4762, Val Acc: 0.8466\n",
      "Epoch [7/10] - Train Loss: 0.4565, Train Acc: 0.8442, Val Loss: 0.4618, Val Acc: 0.8486\n",
      "Epoch [8/10] - Train Loss: 0.4392, Train Acc: 0.8491, Val Loss: 0.4556, Val Acc: 0.8506\n",
      "Epoch [9/10] - Train Loss: 0.4259, Train Acc: 0.8547, Val Loss: 0.4373, Val Acc: 0.8552\n",
      "Epoch [10/10] - Train Loss: 0.4168, Train Acc: 0.8558, Val Loss: 0.4312, Val Acc: 0.8606\n"
     ]
    }
   ],
   "source": [
    "# 기존 코드에서 Training loss, validation loss, Training Accuracy, Validation Accuracy 넣기.version 2\n",
    "# 주석 부분 특히 eval 부분이 추가됨\n",
    "\n",
    "    f_mnist_transforms = nn.Sequential(\n",
    "        transforms.ConvertImageDtype(torch.float),\n",
    "        transforms.Normalize(mean=0.28604060411453247, std=0.3530242443084717),\n",
    "    )\n",
    "\n",
    "def train_and_evaluate(epochs, train_data_loader, validation_data_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Define a simple CNN model\n",
    "    model = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(28 * 28, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 10)\n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = {\"batch_size\": 2048, \"learning_rate\": 0.001, \"epochs\": 10}\n",
    "    wandb.init(mode=\"disabled\", config=config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4db1f647",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/work/DL\n",
      "Num Train Samples:  55000\n",
      "Num Validation Samples:  5000\n",
      "Sample Shape:  torch.Size([1, 28, 28])\n",
      "Number of Data Loading Workers: 2\n",
      "\n",
      "Num Test Samples:  10000\n",
      "Sample Shape:  torch.Size([1, 28, 28])\n",
      "Epoch [1/20] - Train Loss: 0.6036, Train Acc: 0.7940, Val Loss: 0.4574, Val Acc: 0.8392\n",
      "Epoch [2/20] - Train Loss: 0.4286, Train Acc: 0.8498, Val Loss: 0.4125, Val Acc: 0.8610\n",
      "Epoch [3/20] - Train Loss: 0.3914, Train Acc: 0.8612, Val Loss: 0.3797, Val Acc: 0.8644\n",
      "Epoch [4/20] - Train Loss: 0.3631, Train Acc: 0.8706, Val Loss: 0.3707, Val Acc: 0.8718\n",
      "Epoch [5/20] - Train Loss: 0.3425, Train Acc: 0.8761, Val Loss: 0.3433, Val Acc: 0.8776\n",
      "Epoch [6/20] - Train Loss: 0.3276, Train Acc: 0.8825, Val Loss: 0.3400, Val Acc: 0.8830\n",
      "Epoch [7/20] - Train Loss: 0.3137, Train Acc: 0.8853, Val Loss: 0.3335, Val Acc: 0.8838\n",
      "Epoch [8/20] - Train Loss: 0.3028, Train Acc: 0.8890, Val Loss: 0.3143, Val Acc: 0.8884\n",
      "Epoch [9/20] - Train Loss: 0.2873, Train Acc: 0.8943, Val Loss: 0.3172, Val Acc: 0.8876\n",
      "Epoch [10/20] - Train Loss: 0.2836, Train Acc: 0.8957, Val Loss: 0.3183, Val Acc: 0.8848\n",
      "Epoch [11/20] - Train Loss: 0.2716, Train Acc: 0.9002, Val Loss: 0.3183, Val Acc: 0.8878\n",
      "Epoch [12/20] - Train Loss: 0.2655, Train Acc: 0.9028, Val Loss: 0.3033, Val Acc: 0.8908\n",
      "Epoch [13/20] - Train Loss: 0.2568, Train Acc: 0.9061, Val Loss: 0.3061, Val Acc: 0.8920\n",
      "Epoch [14/20] - Train Loss: 0.2468, Train Acc: 0.9089, Val Loss: 0.2997, Val Acc: 0.8962\n",
      "Epoch [15/20] - Train Loss: 0.2412, Train Acc: 0.9111, Val Loss: 0.3026, Val Acc: 0.8948\n",
      "Epoch [16/20] - Train Loss: 0.2368, Train Acc: 0.9126, Val Loss: 0.3081, Val Acc: 0.8972\n",
      "Epoch [17/20] - Train Loss: 0.2323, Train Acc: 0.9150, Val Loss: 0.3083, Val Acc: 0.8922\n",
      "Epoch [18/20] - Train Loss: 0.2280, Train Acc: 0.9146, Val Loss: 0.2978, Val Acc: 0.8982\n",
      "Epoch [19/20] - Train Loss: 0.2179, Train Acc: 0.9203, Val Loss: 0.2998, Val Acc: 0.8986\n",
      "Epoch [20/20] - Train Loss: 0.2161, Train Acc: 0.9194, Val Loss: 0.2952, Val Acc: 0.8968\n"
     ]
    }
   ],
   "source": [
    "# 기존 코드에서 Training loss, validation loss, Training Accuracy, Validation Accuracy 넣기.version 2\n",
    "# 주석 부분 특히 eval 부분이 추가됨\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "# Use the current working directory\n",
    "BASE_PATH = Path().resolve()\n",
    "print(BASE_PATH)\n",
    "\n",
    "import sys\n",
    "sys.path.append(str(BASE_PATH))\n",
    "\n",
    "from utils import get_num_cpu_cores, is_linux, is_windows\n",
    "\n",
    "\n",
    "def get_fashion_mnist_data():\n",
    "    data_path = os.path.join(BASE_PATH, \"_00_data\", \"j_fashion_mnist\")\n",
    "\n",
    "    f_mnist_train = datasets.FashionMNIST(data_path, train=True, download=True, transform=transforms.ToTensor())\n",
    "    f_mnist_train, f_mnist_validation = random_split(f_mnist_train, [55_000, 5_000])\n",
    "\n",
    "    print(\"Num Train Samples: \", len(f_mnist_train))\n",
    "    print(\"Num Validation Samples: \", len(f_mnist_validation))\n",
    "    print(\"Sample Shape: \", f_mnist_train[0][0].shape)  # torch.Size([1, 28, 28])\n",
    "\n",
    "    num_data_loading_workers = get_num_cpu_cores() if is_linux() or is_windows() else 0\n",
    "    print(\"Number of Data Loading Workers:\", num_data_loading_workers)\n",
    "\n",
    "    train_data_loader = DataLoader(\n",
    "        dataset=f_mnist_train, batch_size=wandb.config.batch_size, shuffle=True,\n",
    "        pin_memory=True, num_workers=num_data_loading_workers\n",
    "    )\n",
    "\n",
    "    validation_data_loader = DataLoader(\n",
    "        dataset=f_mnist_validation, batch_size=wandb.config.batch_size,\n",
    "        pin_memory=True, num_workers=num_data_loading_workers\n",
    "    )\n",
    "\n",
    "    f_mnist_transforms = nn.Sequential(\n",
    "        transforms.ConvertImageDtype(torch.float),\n",
    "        transforms.Normalize(mean=0.28604060411453247, std=0.3530242443084717),\n",
    "    )\n",
    "\n",
    "    return train_data_loader, validation_data_loader, f_mnist_transforms\n",
    "\n",
    "\n",
    "def get_fashion_mnist_test_data():\n",
    "    data_path = os.path.join(BASE_PATH, \"_00_data\", \"j_fashion_mnist\")\n",
    "\n",
    "    f_mnist_test_images = datasets.FashionMNIST(data_path, train=False, download=True)\n",
    "    f_mnist_test = datasets.FashionMNIST(data_path, train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "    print(\"Num Test Samples: \", len(f_mnist_test))\n",
    "    print(\"Sample Shape: \", f_mnist_test[0][0].shape)  # torch.Size([1, 28, 28])\n",
    "\n",
    "    test_data_loader = DataLoader(dataset=f_mnist_test, batch_size=len(f_mnist_test))\n",
    "\n",
    "    f_mnist_transforms = nn.Sequential(\n",
    "        transforms.ConvertImageDtype(torch.float),\n",
    "        transforms.Normalize(mean=0.28604060411453247, std=0.3530242443084717),\n",
    "    )    \n",
    "\n",
    "    return f_mnist_test_images, test_data_loader, f_mnist_transforms\n",
    "\n",
    "\n",
    "def train_and_evaluate(epochs, train_data_loader, validation_data_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Define a simple CNN model\n",
    "    model = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(28 * 28, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 10)\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=wandb.config.learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss, train_correct, train_total = 0, 0, 0  # Train loss and accuracy tracking 추가\n",
    "        for images, labels in train_data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()  # Loss 누적\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()  # 정확도 계산\n",
    "\n",
    "        train_accuracy = train_correct / train_total  # Train accuracy 계산\n",
    "        train_loss /= len(train_data_loader)  # 평균 Train loss 계산\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0, 0, 0  # Validation loss and accuracy tracking 추가\n",
    "        with torch.no_grad():\n",
    "            for images, labels in validation_data_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()  # Validation loss 누적\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()  # 정확도 계산\n",
    "\n",
    "        val_accuracy = val_correct / val_total  # Validation accuracy 계산\n",
    "        val_loss /= len(validation_data_loader)  # 평균 Validation loss 계산\n",
    "\n",
    "        # Log metrics to wandb\n",
    "        wandb.log({  # Train/Validation loss와 accuracy를 wandb에 기록 추가\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_accuracy\": train_accuracy,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_accuracy\": val_accuracy\n",
    "        })\n",
    "\n",
    "        # Epoch별 결과 출력 추가\n",
    "        print(f\"Epoch [{epoch}/{epochs}] - Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = {\"batch_size\": 128, \"learning_rate\": 0.001, \"epochs\": 20}\n",
    "    wandb.init(mode=\"disabled\", config=config)\n",
    "\n",
    "    train_data_loader, validation_data_loader, f_mnist_transforms = get_fashion_mnist_data()\n",
    "    print()\n",
    "    f_mnist_test_images, test_data_loader, f_mnist_transforms = get_fashion_mnist_test_data()\n",
    "\n",
    "    # Start training and evaluating\n",
    "    train_and_evaluate(wandb.config.epochs, train_data_loader, validation_data_loader)\n",
    "\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5568fcce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/work/DL\n",
      "Num Train Samples:  55000\n",
      "Num Validation Samples:  5000\n",
      "Sample Shape:  torch.Size([1, 28, 28])\n",
      "Number of Data Loading Workers: 2\n",
      "\n",
      "Num Test Samples:  10000\n",
      "Sample Shape:  torch.Size([1, 28, 28])\n",
      "Epoch [1/20] - Train Loss: 0.6739, Train Acc: 0.7685, Val Loss: 0.4751, Val Acc: 0.8356\n",
      "Epoch [2/20] - Train Loss: 0.4654, Train Acc: 0.8360, Val Loss: 0.4149, Val Acc: 0.8574\n",
      "Epoch [3/20] - Train Loss: 0.4175, Train Acc: 0.8501, Val Loss: 0.3976, Val Acc: 0.8588\n",
      "Epoch [4/20] - Train Loss: 0.3933, Train Acc: 0.8587, Val Loss: 0.3900, Val Acc: 0.8614\n",
      "Epoch [5/20] - Train Loss: 0.3794, Train Acc: 0.8623, Val Loss: 0.3723, Val Acc: 0.8666\n",
      "Epoch [6/20] - Train Loss: 0.3631, Train Acc: 0.8682, Val Loss: 0.3624, Val Acc: 0.8720\n",
      "Epoch [7/20] - Train Loss: 0.3525, Train Acc: 0.8713, Val Loss: 0.3692, Val Acc: 0.8688\n",
      "Epoch [8/20] - Train Loss: 0.3428, Train Acc: 0.8749, Val Loss: 0.3493, Val Acc: 0.8786\n",
      "Epoch [9/20] - Train Loss: 0.3371, Train Acc: 0.8777, Val Loss: 0.3438, Val Acc: 0.8754\n",
      "Epoch [10/20] - Train Loss: 0.3294, Train Acc: 0.8795, Val Loss: 0.3479, Val Acc: 0.8786\n",
      "Epoch [11/20] - Train Loss: 0.3217, Train Acc: 0.8818, Val Loss: 0.3375, Val Acc: 0.8806\n",
      "Epoch [12/20] - Train Loss: 0.3157, Train Acc: 0.8849, Val Loss: 0.3411, Val Acc: 0.8816\n",
      "Epoch [13/20] - Train Loss: 0.3099, Train Acc: 0.8865, Val Loss: 0.3377, Val Acc: 0.8808\n",
      "Epoch [14/20] - Train Loss: 0.3069, Train Acc: 0.8864, Val Loss: 0.3367, Val Acc: 0.8814\n",
      "Epoch [15/20] - Train Loss: 0.3017, Train Acc: 0.8904, Val Loss: 0.3281, Val Acc: 0.8850\n",
      "Epoch [16/20] - Train Loss: 0.2971, Train Acc: 0.8890, Val Loss: 0.3272, Val Acc: 0.8880\n",
      "Epoch [17/20] - Train Loss: 0.2913, Train Acc: 0.8921, Val Loss: 0.3286, Val Acc: 0.8864\n",
      "Epoch [18/20] - Train Loss: 0.2913, Train Acc: 0.8921, Val Loss: 0.3254, Val Acc: 0.8882\n",
      "Epoch [19/20] - Train Loss: 0.2826, Train Acc: 0.8948, Val Loss: 0.3243, Val Acc: 0.8856\n",
      "Epoch [20/20] - Train Loss: 0.2835, Train Acc: 0.8950, Val Loss: 0.3223, Val Acc: 0.8912\n"
     ]
    }
   ],
   "source": [
    "# 기존 코드에서 Training loss, validation loss, Training Accuracy, Validation Accuracy 넣기.version 2\n",
    "# dropout 추가함\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "# Use the current working directory\n",
    "BASE_PATH = Path().resolve()\n",
    "print(BASE_PATH)\n",
    "\n",
    "import sys\n",
    "sys.path.append(str(BASE_PATH))\n",
    "\n",
    "from utils import get_num_cpu_cores, is_linux, is_windows\n",
    "\n",
    "\n",
    "def get_fashion_mnist_data():\n",
    "    data_path = os.path.join(BASE_PATH, \"_00_data\", \"j_fashion_mnist\")\n",
    "\n",
    "    f_mnist_train = datasets.FashionMNIST(data_path, train=True, download=True, transform=transforms.ToTensor())\n",
    "    f_mnist_train, f_mnist_validation = random_split(f_mnist_train, [55_000, 5_000])\n",
    "\n",
    "    print(\"Num Train Samples: \", len(f_mnist_train))\n",
    "    print(\"Num Validation Samples: \", len(f_mnist_validation))\n",
    "    print(\"Sample Shape: \", f_mnist_train[0][0].shape)  # torch.Size([1, 28, 28])\n",
    "\n",
    "    num_data_loading_workers = get_num_cpu_cores() if is_linux() or is_windows() else 0\n",
    "    print(\"Number of Data Loading Workers:\", num_data_loading_workers)\n",
    "\n",
    "    train_data_loader = DataLoader(\n",
    "        dataset=f_mnist_train, batch_size=wandb.config.batch_size, shuffle=True,\n",
    "        pin_memory=True, num_workers=num_data_loading_workers\n",
    "    )\n",
    "\n",
    "    validation_data_loader = DataLoader(\n",
    "        dataset=f_mnist_validation, batch_size=wandb.config.batch_size,\n",
    "        pin_memory=True, num_workers=num_data_loading_workers\n",
    "    )\n",
    "\n",
    "    f_mnist_transforms = nn.Sequential(\n",
    "        transforms.ConvertImageDtype(torch.float),\n",
    "        transforms.Normalize(mean=0.28604060411453247, std=0.3530242443084717),\n",
    "        #추가\n",
    "        nn.Flatten(),\n",
    "    )\n",
    "\n",
    "    return train_data_loader, validation_data_loader, f_mnist_transforms\n",
    "\n",
    "\n",
    "def get_fashion_mnist_test_data():\n",
    "    data_path = os.path.join(BASE_PATH, \"_00_data\", \"j_fashion_mnist\")\n",
    "\n",
    "    f_mnist_test_images = datasets.FashionMNIST(data_path, train=False, download=True)\n",
    "    f_mnist_test = datasets.FashionMNIST(data_path, train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "    print(\"Num Test Samples: \", len(f_mnist_test))\n",
    "    print(\"Sample Shape: \", f_mnist_test[0][0].shape)  # torch.Size([1, 28, 28])\n",
    "\n",
    "    test_data_loader = DataLoader(dataset=f_mnist_test, batch_size=len(f_mnist_test))\n",
    "\n",
    "    f_mnist_transforms = nn.Sequential(\n",
    "        transforms.ConvertImageDtype(torch.float),\n",
    "        transforms.Normalize(mean=0.28604060411453247, std=0.3530242443084717),\n",
    "        #추가\n",
    "        nn.Flatten(),\n",
    "    )    \n",
    "\n",
    "    return f_mnist_test_images, test_data_loader, f_mnist_transforms\n",
    "\n",
    "\n",
    "def train_and_evaluate(epochs, train_data_loader, validation_data_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Define a simple CNN model\n",
    "    model = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(28 * 28, 128),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(128, 10)\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=wandb.config.learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss, train_correct, train_total = 0, 0, 0  # Train loss and accuracy tracking 추가\n",
    "        for images, labels in train_data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()  # Loss 누적\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()  # 정확도 계산\n",
    "\n",
    "        train_accuracy = train_correct / train_total  # Train accuracy 계산\n",
    "        train_loss /= len(train_data_loader)  # 평균 Train loss 계산\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0, 0, 0  # Validation loss and accuracy tracking 추가\n",
    "        with torch.no_grad():\n",
    "            for images, labels in validation_data_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()  # Validation loss 누적\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()  # 정확도 계산\n",
    "\n",
    "        val_accuracy = val_correct / val_total  # Validation accuracy 계산\n",
    "        val_loss /= len(validation_data_loader)  # 평균 Validation loss 계산\n",
    "\n",
    "        # Log metrics to wandb\n",
    "        wandb.log({  # Train/Validation loss와 accuracy를 wandb에 기록 추가\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_accuracy\": train_accuracy,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_accuracy\": val_accuracy\n",
    "        })\n",
    "\n",
    "        # Epoch별 결과 출력 추가\n",
    "        print(f\"Epoch [{epoch}/{epochs}] - Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "# 외부 코드 추가\n",
    "from torch.optim import Adam\n",
    "\n",
    "# 외부 코드 추가\n",
    "# Define CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(64 * 14 * 14, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = {\"batch_size\": 128, \"learning_rate\": 0.001, \"epochs\": 20}\n",
    "    wandb.init(mode=\"disabled\", config=config)\n",
    "\n",
    "    train_data_loader, validation_data_loader, f_mnist_transforms = get_fashion_mnist_data()\n",
    "    print()\n",
    "    f_mnist_test_images, test_data_loader, f_mnist_transforms = get_fashion_mnist_test_data()\n",
    "# 외부 코드 추가\n",
    "    # Model, Optimizer, Criterion\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = SimpleCNN().to(device)\n",
    "#     외부 코드 원본\n",
    "#     optimizer = Adam(model.parameters(), lr=config.learning_rate)\n",
    "    optimizer = Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "# 추가\n",
    "    # Train and Validate\n",
    "#    외부 코드 원본 \n",
    "#     train_and_validate(model, train_loader, validation_loader, config.epochs, device, optimizer, criterion)\n",
    "#     train_and_evaluate(model, train_data_loader, validation_data_loader, config[\"epochs\"])\n",
    "\n",
    "    # Start training and evaluating\n",
    "    # 원래 원본\n",
    "    train_and_evaluate(wandb.config.epochs, train_data_loader, validation_data_loader)\n",
    "\n",
    "    wandb.finish()\n",
    "    \n",
    "    \n",
    "# 외부 코드\n",
    "# Get Data Loaders\n",
    "#     train_loader, validation_loader = get_fashion_mnist_data(config.batch_size)\n",
    "\n",
    "#     # Model, Optimizer, Criterion\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     model = SimpleCNN().to(device)\n",
    "#     optimizer = Adam(model.parameters(), lr=config.learning_rate)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#     # Train and Validate\n",
    "#     train_and_validate(model, train_loader, validation_loader, config.epochs, device, optimizer, criterion)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f21e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 코드에서 Training loss, validation loss, Training Accuracy, Validation Accuracy 넣기.version 2\n",
    "# dropout 추가함\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "# Use the current working directory\n",
    "BASE_PATH = Path().resolve()\n",
    "print(BASE_PATH)\n",
    "\n",
    "import sys\n",
    "sys.path.append(str(BASE_PATH))\n",
    "\n",
    "from utils import get_num_cpu_cores, is_linux, is_windows\n",
    "\n",
    "\n",
    "def get_fashion_mnist_data():\n",
    "    data_path = os.path.join(BASE_PATH, \"_00_data\", \"j_fashion_mnist\")\n",
    "\n",
    "    f_mnist_train = datasets.FashionMNIST(data_path, train=True, download=True, transform=transforms.ToTensor())\n",
    "    f_mnist_train, f_mnist_validation = random_split(f_mnist_train, [55_000, 5_000])\n",
    "\n",
    "    print(\"Num Train Samples: \", len(f_mnist_train))\n",
    "    print(\"Num Validation Samples: \", len(f_mnist_validation))\n",
    "    print(\"Sample Shape: \", f_mnist_train[0][0].shape)  # torch.Size([1, 28, 28])\n",
    "\n",
    "    num_data_loading_workers = get_num_cpu_cores() if is_linux() or is_windows() else 0\n",
    "    print(\"Number of Data Loading Workers:\", num_data_loading_workers)\n",
    "\n",
    "    train_data_loader = DataLoader(\n",
    "        dataset=f_mnist_train, batch_size=wandb.config.batch_size, shuffle=True,\n",
    "        pin_memory=True, num_workers=num_data_loading_workers\n",
    "    )\n",
    "\n",
    "    validation_data_loader = DataLoader(\n",
    "        dataset=f_mnist_validation, batch_size=wandb.config.batch_size,\n",
    "        pin_memory=True, num_workers=num_data_loading_workers\n",
    "    )\n",
    "\n",
    "    f_mnist_transforms = nn.Sequential(\n",
    "        transforms.ConvertImageDtype(torch.float),\n",
    "        transforms.Normalize(mean=0.286, std=0.353),\n",
    "        #추가\n",
    "        nn.Flatten(),\n",
    "    )\n",
    "\n",
    "    return train_data_loader, validation_data_loader, f_mnist_transforms\n",
    "\n",
    "\n",
    "def get_fashion_mnist_test_data():\n",
    "    data_path = os.path.join(BASE_PATH, \"_00_data\", \"j_fashion_mnist\")\n",
    "\n",
    "    f_mnist_test_images = datasets.FashionMNIST(data_path, train=False, download=True)\n",
    "    f_mnist_test = datasets.FashionMNIST(data_path, train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "    print(\"Num Test Samples: \", len(f_mnist_test))\n",
    "    print(\"Sample Shape: \", f_mnist_test[0][0].shape)  # torch.Size([1, 28, 28])\n",
    "\n",
    "    test_data_loader = DataLoader(dataset=f_mnist_test, batch_size=len(f_mnist_test))\n",
    "\n",
    "    f_mnist_transforms = nn.Sequential(\n",
    "        transforms.ConvertImageDtype(torch.float),\n",
    "        transforms.Normalize(mean=0.286, std=0.353),\n",
    "        #추가\n",
    "        nn.Flatten(),\n",
    "    )    \n",
    "\n",
    "    return f_mnist_test_images, test_data_loader, f_mnist_transforms\n",
    "\n",
    "\n",
    "def train_and_evaluate(epochs, train_data_loader, validation_data_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Define a simple CNN model\n",
    "    #model = nn.Sequential(\n",
    "       # nn.Flatten(),\n",
    "      #  nn.Linear(28 * 28, 128),\n",
    "      #  nn.Dropout(0.3),\n",
    "      #  nn.ReLU(),\n",
    "       # nn.Dropout(0.3),\n",
    "      #  nn.Linear(128, 10)\n",
    "  #  ).to(device)\n",
    "    \n",
    " # Define a simple CNN model Val : 93\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(1, 32, kernel_size=3, padding=1),  # Input: (1, 28, 28), Output: (32, 28, 28)\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),       # Output: (32, 14, 14)\n",
    "\n",
    "        nn.Conv2d(32, 64, kernel_size=3, padding=1), # Output: (64, 14, 14)\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),       # Output: (64, 7, 7)\n",
    "\n",
    "        nn.Flatten(),                                # Flatten for Fully Connected Layer\n",
    "        nn.Linear(64 * 7 * 7, 128),                 # Fully Connected Layer\n",
    "        nn.Dropout(0.3),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 10)                          # Output Layer for 10 Classes\n",
    "    ).to(device)\n",
    "   \n",
    "    \n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=wandb.config.learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss, train_correct, train_total = 0, 0, 0  # Train loss and accuracy tracking 추가\n",
    "        for images, labels in train_data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()  # Loss 누적\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()  # 정확도 계산\n",
    "\n",
    "        train_accuracy = train_correct / train_total  # Train accuracy 계산\n",
    "        train_loss /= len(train_data_loader)  # 평균 Train loss 계산\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0, 0, 0  # Validation loss and accuracy tracking 추가\n",
    "        with torch.no_grad():\n",
    "            for images, labels in validation_data_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()  # Validation loss 누적\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()  # 정확도 계산\n",
    "\n",
    "        val_accuracy = val_correct / val_total  # Validation accuracy 계산\n",
    "        val_loss /= len(validation_data_loader)  # 평균 Validation loss 계산\n",
    "\n",
    "        # Log metrics to wandb\n",
    "        wandb.log({  # Train/Validation loss와 accuracy를 wandb에 기록 추가\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_accuracy\": train_accuracy,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_accuracy\": val_accuracy\n",
    "        })\n",
    "\n",
    "        # Epoch별 결과 출력 추가\n",
    "        print(f\"Epoch [{epoch}/{epochs}] - Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "# 외부 코드 추가\n",
    "from torch.optim import Adam\n",
    "\n",
    "# 외부 코드 추가\n",
    "# Define CNN model \n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(64 * 14 * 14, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = {\"batch_size\": 128, \"learning_rate\": 0.001, \"epochs\": 25}\n",
    "    wandb.init(mode=\"disabled\", config=config)\n",
    "\n",
    "    train_data_loader, validation_data_loader, f_mnist_transforms = get_fashion_mnist_data()\n",
    "    print()\n",
    "    f_mnist_test_images, test_data_loader, f_mnist_transforms = get_fashion_mnist_test_data()\n",
    "# 외부 코드 추가\n",
    "    # Model, Optimizer, Criterion\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = SimpleCNN().to(device)\n",
    "#     외부 코드 원본\n",
    "#     optimizer = Adam(model.parameters(), lr=config.learning_rate)\n",
    "    optimizer = Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "# 추가\n",
    "    # Train and Validate\n",
    "#    외부 코드 원본 \n",
    "#     train_and_validate(model, train_loader, validation_loader, config.epochs, device, optimizer, criterion)\n",
    "#     train_and_evaluate(model, train_data_loader, validation_data_loader, config[\"epochs\"])\n",
    "\n",
    "    # Start training and evaluating\n",
    "    # 원래 원본\n",
    "    train_and_evaluate(wandb.config.epochs, train_data_loader, validation_data_loader)\n",
    "\n",
    "    wandb.finish()\n",
    "    \n",
    "    \n",
    "# 외부 코드\n",
    "# Get Data Loaders\n",
    "#     train_loader, validation_loader = get_fashion_mnist_data(config.batch_size)\n",
    "\n",
    "#     # Model, Optimizer, Criterion\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     model = SimpleCNN().to(device)\n",
    "#     optimizer = Adam(model.parameters(), lr=config.learning_rate)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#     # Train and Validate\n",
    "#     train_and_validate(model, train_loader, validation_loader, config.epochs, device, optimizer, criterion)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dd996e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 코드에서 Training loss, validation loss, Training Accuracy, Validation Accuracy 넣기.version 2\n",
    "# dropout 추가함\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "# Use the current working directory\n",
    "BASE_PATH = Path().resolve()\n",
    "print(BASE_PATH)\n",
    "\n",
    "import sys\n",
    "sys.path.append(str(BASE_PATH))\n",
    "\n",
    "from utils import get_num_cpu_cores, is_linux, is_windows\n",
    "\n",
    "\n",
    "def get_fashion_mnist_data():\n",
    "    data_path = os.path.join(BASE_PATH, \"_00_data\", \"j_fashion_mnist\")\n",
    "\n",
    "    f_mnist_train = datasets.FashionMNIST(data_path, train=True, download=True, transform=transforms.ToTensor())\n",
    "    f_mnist_train, f_mnist_validation = random_split(f_mnist_train, [55_000, 5_000])\n",
    "\n",
    "    print(\"Num Train Samples: \", len(f_mnist_train))\n",
    "    print(\"Num Validation Samples: \", len(f_mnist_validation))\n",
    "    print(\"Sample Shape: \", f_mnist_train[0][0].shape)  # torch.Size([1, 28, 28])\n",
    "\n",
    "    num_data_loading_workers = get_num_cpu_cores() if is_linux() or is_windows() else 0\n",
    "    print(\"Number of Data Loading Workers:\", num_data_loading_workers)\n",
    "\n",
    "    train_data_loader = DataLoader(\n",
    "        dataset=f_mnist_train, batch_size=wandb.config.batch_size, shuffle=True,\n",
    "        pin_memory=True, num_workers=num_data_loading_workers\n",
    "    )\n",
    "\n",
    "    validation_data_loader = DataLoader(\n",
    "        dataset=f_mnist_validation, batch_size=wandb.config.batch_size,\n",
    "        pin_memory=True, num_workers=num_data_loading_workers\n",
    "    )\n",
    "\n",
    "    f_mnist_transforms = nn.Sequential(\n",
    "        transforms.ConvertImageDtype(torch.float),\n",
    "        transforms.Normalize(mean=0.286, std=0.353),\n",
    "        #추가\n",
    "        nn.Flatten(),\n",
    "    )\n",
    "\n",
    "    return train_data_loader, validation_data_loader, f_mnist_transforms\n",
    "\n",
    "\n",
    "def get_fashion_mnist_test_data():\n",
    "    data_path = os.path.join(BASE_PATH, \"_00_data\", \"j_fashion_mnist\")\n",
    "\n",
    "    f_mnist_test_images = datasets.FashionMNIST(data_path, train=False, download=True)\n",
    "    f_mnist_test = datasets.FashionMNIST(data_path, train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "    print(\"Num Test Samples: \", len(f_mnist_test))\n",
    "    print(\"Sample Shape: \", f_mnist_test[0][0].shape)  # torch.Size([1, 28, 28])\n",
    "\n",
    "    test_data_loader = DataLoader(dataset=f_mnist_test, batch_size=len(f_mnist_test))\n",
    "\n",
    "    f_mnist_transforms = nn.Sequential(\n",
    "        transforms.ConvertImageDtype(torch.float),\n",
    "        transforms.Normalize(mean=0.286, std=0.353),\n",
    "        #추가\n",
    "        nn.Flatten(),\n",
    "    )    \n",
    "\n",
    "    return f_mnist_test_images, test_data_loader, f_mnist_transforms\n",
    "\n",
    "\n",
    "def train_and_evaluate(epochs, train_data_loader, validation_data_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Define a simple CNN model\n",
    "    #model = nn.Sequential(\n",
    "       # nn.Flatten(),\n",
    "      #  nn.Linear(28 * 28, 128),\n",
    "      #  nn.Dropout(0.3),\n",
    "      #  nn.ReLU(),\n",
    "       # nn.Dropout(0.3),\n",
    "      #  nn.Linear(128, 10)\n",
    "  #  ).to(device)\n",
    "    \n",
    " # Define a simple CNN model Val : 93\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(1, 32, kernel_size=3, padding=1),  # Input: (1, 28, 28), Output: (32, 28, 28)\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),       # Output: (32, 14, 14)\n",
    "\n",
    "        nn.Conv2d(32, 64, kernel_size=3, padding=1), # Output: (64, 14, 14)\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),       # Output: (64, 7, 7)\n",
    "\n",
    "        nn.Flatten(),                                # Flatten for Fully Connected Layer\n",
    "        nn.Linear(64 * 7 * 7, 128),                 # Fully Connected Layer\n",
    "        nn.Dropout(0.3),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 10)                          # Output Layer for 10 Classes\n",
    "    ).to(device)\n",
    "\n",
    "    # Define LeNet-5 model val이 높지 않음\n",
    "    #model = nn.Sequential(\n",
    "     #   nn.Conv2d(1, 6, kernel_size=5),             # Input: (1, 28, 28), Output: (6, 24, 24)\n",
    "       # nn.ReLU(),\n",
    "       # nn.AvgPool2d(kernel_size=2, stride=2),      # Output: (6, 12, 12)\n",
    "\n",
    "     #   nn.Conv2d(6, 16, kernel_size=5),            # Output: (16, 8, 8)\n",
    "     #   nn.ReLU(),\n",
    "       # nn.AvgPool2d(kernel_size=2, stride=2),      # Output: (16, 4, 4)\n",
    "\n",
    "     #   nn.Flatten(),                               # Flatten for Fully Connected Layer\n",
    "     #   nn.Linear(16 * 4 * 4, 120),                # Fully Connected Layer\n",
    "     #   nn.ReLU(),\n",
    "     #   nn.Linear(120, 84),                        # Fully Connected Layer\n",
    "     #   nn.ReLU(),\n",
    "     #   nn.Linear(84, 10)                          # Output Layer for 10 Classes\n",
    "   # ).to(device)\n",
    "\n",
    "   # Define a deeper Fully Connected Network # Validation:90\n",
    "  #  model = nn.Sequential(\n",
    "  #      nn.Flatten(),\n",
    "  #      nn.Linear(28 * 28, 256),\n",
    "  #      nn.Dropout(0.3),\n",
    "  #      nn.ReLU(),\n",
    " #       nn.Linear(256, 128),\n",
    "   #     nn.Dropout(0.3),\n",
    "  #      nn.ReLU(),\n",
    "  #      nn.Linear(128, 64),\n",
    "   #     nn.Dropout(0.3),\n",
    " #       nn.ReLU(),\n",
    " #       nn.Linear(64, 10)\n",
    "#    ).to(device)\n",
    "\n",
    "   # Replace the model definition in train_and_evaluate function \n",
    "    # val : 92.3\n",
    " #   model = nn.Sequential(\n",
    "     #   nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "  #      nn.ReLU(),\n",
    "   #     nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    #    nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "   #     nn.ReLU(),\n",
    "   #     nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    #    nn.Flatten(),\n",
    "   #     nn.Linear(64 * 7 * 7, 128),\n",
    "    #    nn.Dropout(0.3),\n",
    "    #    nn.ReLU(),\n",
    "    #    nn.Linear(128, 10)\n",
    "  #  ).to(device)\n",
    " \n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=wandb.config.learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss, train_correct, train_total = 0, 0, 0  # Train loss and accuracy tracking 추가\n",
    "        for images, labels in train_data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()  # Loss 누적\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()  # 정확도 계산\n",
    "\n",
    "        train_accuracy = train_correct / train_total  # Train accuracy 계산\n",
    "        train_loss /= len(train_data_loader)  # 평균 Train loss 계산\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0, 0, 0  # Validation loss and accuracy tracking 추가\n",
    "        with torch.no_grad():\n",
    "            for images, labels in validation_data_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()  # Validation loss 누적\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()  # 정확도 계산\n",
    "\n",
    "        val_accuracy = val_correct / val_total  # Validation accuracy 계산\n",
    "        val_loss /= len(validation_data_loader)  # 평균 Validation loss 계산\n",
    "\n",
    "        # Log metrics to wandb\n",
    "        wandb.log({  # Train/Validation loss와 accuracy를 wandb에 기록 추가\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_accuracy\": train_accuracy,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_accuracy\": val_accuracy\n",
    "        })\n",
    "\n",
    "        # Epoch별 결과 출력 추가\n",
    "        print(f\"Epoch [{epoch}/{epochs}] - Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "# 외부 코드 추가\n",
    "from torch.optim import Adam\n",
    "\n",
    "# 외부 코드 추가\n",
    "# Define CNN model \n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(64 * 14 * 14, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = {\"batch_size\": 64, \"learning_rate\": 0.0005, \"epochs\": 25}\n",
    "    wandb.init(mode=\"disabled\", config=config)\n",
    "\n",
    "    train_data_loader, validation_data_loader, f_mnist_transforms = get_fashion_mnist_data()\n",
    "    print()\n",
    "    f_mnist_test_images, test_data_loader, f_mnist_transforms = get_fashion_mnist_test_data()\n",
    "# 외부 코드 추가\n",
    "    # Model, Optimizer, Criterion\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = SimpleCNN().to(device)\n",
    "#     외부 코드 원본\n",
    "#     optimizer = Adam(model.parameters(), lr=config.learning_rate)\n",
    "    optimizer = Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "# 추가\n",
    "    # Train and Validate\n",
    "#    외부 코드 원본 \n",
    "#     train_and_validate(model, train_loader, validation_loader, config.epochs, device, optimizer, criterion)\n",
    "#     train_and_evaluate(model, train_data_loader, validation_data_loader, config[\"epochs\"])\n",
    "\n",
    "    # Start training and evaluating\n",
    "    # 원래 원본\n",
    "    train_and_evaluate(wandb.config.epochs, train_data_loader, validation_data_loader)\n",
    "\n",
    "    wandb.finish()\n",
    "    \n",
    "    \n",
    "# 외부 코드\n",
    "# Get Data Loaders\n",
    "#     train_loader, validation_loader = get_fashion_mnist_data(config.batch_size)\n",
    "\n",
    "#     # Model, Optimizer, Criterion\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     model = SimpleCNN().to(device)\n",
    "#     optimizer = Adam(model.parameters(), lr=config.learning_rate)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#     # Train and Validate\n",
    "#     train_and_validate(model, train_loader, validation_loader, config.epochs, device, optimizer, criterion)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2425dff4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 최신 2\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import Compose, RandomHorizontalFlip, RandomRotation, transforms\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Use the current working directory\n",
    "BASE_PATH = Path().resolve()\n",
    "\n",
    "def get_fashion_mnist_data():\n",
    "    data_path = os.path.join(BASE_PATH, \"_00_data\", \"j_fashion_mnist\")\n",
    "\n",
    "    transform = Compose([\n",
    "        transforms.ToTensor(),\n",
    "        RandomHorizontalFlip(p=0.5),  # 50% 확률로 이미지를 좌우 반전\n",
    "        RandomRotation(10),          # 이미지를 -10도에서 10도 사이로 회전\n",
    "        transforms.Normalize(mean=0.286, std=0.353)\n",
    "    ])\n",
    "\n",
    "    f_mnist_train = datasets.FashionMNIST(data_path, train=True, download=True, transform=transform)\n",
    "    f_mnist_train, f_mnist_validation = random_split(f_mnist_train, [55_000, 5_000])\n",
    "\n",
    "    train_data_loader = DataLoader(\n",
    "        dataset=f_mnist_train, batch_size=wandb.config.batch_size, shuffle=True,\n",
    "        pin_memory=True, num_workers=0\n",
    "    )\n",
    "\n",
    "    validation_data_loader = DataLoader(\n",
    "        dataset=f_mnist_validation, batch_size=wandb.config.batch_size,\n",
    "        pin_memory=True, num_workers=0\n",
    "    )\n",
    "\n",
    "    return train_data_loader, validation_data_loader\n",
    "\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        log_probs = torch.nn.functional.log_softmax(outputs, dim=-1)\n",
    "        targets = torch.nn.functional.one_hot(targets, num_classes=outputs.size(-1))\n",
    "        targets = targets * (1 - self.smoothing) + self.smoothing / outputs.size(-1)\n",
    "        return -(targets * log_probs).sum(dim=-1).mean()\n",
    "\n",
    "def train_and_evaluate(epochs, train_data_loader, validation_data_loader):\n",
    "    best_val_accuracy = 0\n",
    "    patience = 5  # 성능 향상이 없을 경우 학습 중단까지 기다리는 epoch 수\n",
    "    trigger_times = 0\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(32),  # Batch Normalization\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "        nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(64),  # Batch Normalization\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(64 * 7 * 7, 128),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 10)\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=wandb.config.learning_rate)\n",
    "    scheduler = StepLR(optimizer, step_size=10, gamma=0.1)  # 매 10 epoch마다 학습률 10% 감소\n",
    "    criterion = LabelSmoothingCrossEntropy(smoothing=0.1)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        train_loss, train_correct, train_total = 0, 0, 0\n",
    "        for images, labels in train_data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        scheduler.step()  # 학습률 업데이트\n",
    "\n",
    "        train_accuracy = train_correct / train_total\n",
    "        train_loss /= len(train_data_loader)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in validation_data_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_accuracy = val_correct / val_total\n",
    "        val_loss /= len(validation_data_loader)\n",
    "\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            trigger_times = 0\n",
    "        else:\n",
    "            trigger_times += 1\n",
    "        if trigger_times >= patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_accuracy\": train_accuracy,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_accuracy\": val_accuracy\n",
    "        })\n",
    "\n",
    "        print(f\"Epoch [{epoch}/{epochs}] - Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = {\"batch_size\": 64, \"learning_rate\": 1e-3, \"epochs\": 25}\n",
    "    wandb.init(mode=\"disabled\", config=config)\n",
    "\n",
    "    train_data_loader, validation_data_loader = get_fashion_mnist_data()\n",
    "    train_and_evaluate(config[\"epochs\"], train_data_loader, validation_data_loader)\n",
    "    wandb.finish()\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b316504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최신 3 ResNet18\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, models\n",
    "from torchvision.transforms import transforms\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Use the current working directory\n",
    "BASE_PATH = Path().resolve()\n",
    "print(BASE_PATH)\n",
    "\n",
    "import sys\n",
    "sys.path.append(str(BASE_PATH))\n",
    "\n",
    "from utils import get_num_cpu_cores, is_linux, is_windows\n",
    "\n",
    "\n",
    "def get_fashion_mnist_data():\n",
    "    data_path = os.path.join(BASE_PATH, \"_00_data\", \"j_fashion_mnist\")\n",
    "\n",
    "    f_mnist_train = datasets.FashionMNIST(data_path, train=True, download=True, transform=transforms.ToTensor())\n",
    "    f_mnist_train, f_mnist_validation = random_split(f_mnist_train, [55_000, 5_000])\n",
    "\n",
    "    print(\"Num Train Samples: \", len(f_mnist_train))\n",
    "    print(\"Num Validation Samples: \", len(f_mnist_validation))\n",
    "    print(\"Sample Shape: \", f_mnist_train[0][0].shape)  # torch.Size([1, 28, 28])\n",
    "\n",
    "    num_data_loading_workers = get_num_cpu_cores() if is_linux() or is_windows() else 0\n",
    "    print(\"Number of Data Loading Workers:\", num_data_loading_workers)\n",
    "\n",
    "    train_data_loader = DataLoader(\n",
    "        dataset=f_mnist_train, batch_size=wandb.config.batch_size, shuffle=True,\n",
    "        pin_memory=True, num_workers=num_data_loading_workers\n",
    "    )\n",
    "\n",
    "    validation_data_loader = DataLoader(\n",
    "        dataset=f_mnist_validation, batch_size=wandb.config.batch_size,\n",
    "        pin_memory=True, num_workers=num_data_loading_workers\n",
    "    )\n",
    "\n",
    "    return train_data_loader, validation_data_loader\n",
    "\n",
    "\n",
    "def train_and_evaluate(epochs, train_data_loader, validation_data_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load ResNet18 and modify for FashionMNIST\n",
    "    model = models.resnet18(pretrained=False)  # Load ResNet18 without pre-trained weights\n",
    "    model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)  # Modify for single-channel input\n",
    "    model.fc = nn.Linear(model.fc.in_features, 10)  # Modify the output layer for 10 classes\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=wandb.config.learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss, train_correct, train_total = 0, 0, 0\n",
    "        for images, labels in train_data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_accuracy = train_correct / train_total\n",
    "        train_loss /= len(train_data_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in validation_data_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_accuracy = val_correct / val_total\n",
    "        val_loss /= len(validation_data_loader)\n",
    "\n",
    "        # Log metrics to wandb\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_accuracy\": train_accuracy,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_accuracy\": val_accuracy\n",
    "        })\n",
    "\n",
    "        print(f\"Epoch [{epoch}/{epochs}] - Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = {\"batch_size\": 64, \"learning_rate\": 0.0005, \"epochs\": 25}\n",
    "    wandb.init(mode=\"disabled\", config=config)\n",
    "\n",
    "    train_data_loader, validation_data_loader = get_fashion_mnist_data()\n",
    "\n",
    "    # Start training and evaluating\n",
    "    train_and_evaluate(wandb.config.epochs, train_data_loader, validation_data_loader)\n",
    "\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "729366c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/work/DL\n",
      "Num Train Samples:  55000\n",
      "Num Validation Samples:  5000\n",
      "Sample Shape:  torch.Size([1, 28, 28])\n",
      "Number of Data Loading Workers: 2\n",
      "\n",
      "Num Test Samples:  10000\n",
      "Sample Shape:  torch.Size([1, 28, 28])\n",
      "Epoch [1/20] - Train Loss: 1.9108, Train Acc: 0.6216, Val Loss: 1.8584, Val Acc: 0.6692 | Early stopping is stated!\n",
      "Epoch [2/20] - Train Loss: 1.8577, Train Acc: 0.6669, Val Loss: 1.8531, Val Acc: 0.6676 | V_loss decreased (1.85839 --> 1.85311). Saving model...\n",
      "Epoch [3/20] - Train Loss: 1.8544, Train Acc: 0.6692, Val Loss: 1.8506, Val Acc: 0.6662 | V_loss decreased (1.85311 --> 1.85064). Saving model...\n",
      "Epoch [4/20] - Train Loss: 1.8540, Train Acc: 0.6697, Val Loss: 1.8502, Val Acc: 0.6664 | Early stopping counter: 1 out of 5\n",
      "Epoch [5/20] - Train Loss: 1.8521, Train Acc: 0.6725, Val Loss: 1.8469, Val Acc: 0.6696 | V_loss decreased (1.85064 --> 1.84688). Saving model...\n",
      "Epoch [6/20] - Train Loss: 1.8518, Train Acc: 0.6711, Val Loss: 1.8495, Val Acc: 0.6732 | Early stopping counter: 1 out of 5\n",
      "Epoch [7/20] - Train Loss: 1.8513, Train Acc: 0.6731, Val Loss: 1.8568, Val Acc: 0.6578 | Early stopping counter: 2 out of 5\n",
      "Epoch [8/20] - Train Loss: 1.8506, Train Acc: 0.6746, Val Loss: 1.8484, Val Acc: 0.6724 | Early stopping counter: 3 out of 5\n",
      "Epoch [9/20] - Train Loss: 1.8510, Train Acc: 0.6734, Val Loss: 1.8473, Val Acc: 0.6784 | Early stopping counter: 4 out of 5\n",
      "Epoch [10/20] - Train Loss: 1.8501, Train Acc: 0.6753, Val Loss: 1.8479, Val Acc: 0.6702 | Early stopping counter: 5 out of 5 *** TRAIN EARLY STOPPED! ***\n",
      "Early stopping triggered. Stopping training.\n",
      "Final training time: 0:00:32.224560\n"
     ]
    }
   ],
   "source": [
    "# 기존 코드에서 Training loss, validation loss, Training Accuracy, Validation Accuracy 넣기.version 2\n",
    "# dropout 추가함\n",
    "# early patience 추가\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "from early_stopping import EarlyStopping  \n",
    "\n",
    "# Use the current working directory\n",
    "BASE_PATH = Path().resolve()\n",
    "print(BASE_PATH)\n",
    "\n",
    "# 체크포인트 디렉터리 생성\n",
    "os.makedirs(BASE_PATH / \"checkpoints\", exist_ok=True)\n",
    "\n",
    "import sys\n",
    "sys.path.append(str(BASE_PATH))\n",
    "\n",
    "from utils import get_num_cpu_cores, is_linux, is_windows\n",
    "\n",
    "\n",
    "\n",
    "def get_fashion_mnist_data():\n",
    "    data_path = os.path.join(BASE_PATH, \"_00_data\", \"j_fashion_mnist\")\n",
    "\n",
    "    f_mnist_train = datasets.FashionMNIST(data_path, train=True, download=True, transform=transforms.ToTensor())\n",
    "    f_mnist_train, f_mnist_validation = random_split(f_mnist_train, [55_000, 5_000])\n",
    "\n",
    "    print(\"Num Train Samples: \", len(f_mnist_train))\n",
    "    print(\"Num Validation Samples: \", len(f_mnist_validation))\n",
    "    print(\"Sample Shape: \", f_mnist_train[0][0].shape)  # torch.Size([1, 28, 28])\n",
    "\n",
    "    num_data_loading_workers = get_num_cpu_cores() if is_linux() or is_windows() else 0\n",
    "    print(\"Number of Data Loading Workers:\", num_data_loading_workers)\n",
    "\n",
    "    train_data_loader = DataLoader(\n",
    "        dataset=f_mnist_train, batch_size=wandb.config.batch_size, shuffle=True,\n",
    "        pin_memory=True, num_workers=num_data_loading_workers\n",
    "    )\n",
    "\n",
    "    validation_data_loader = DataLoader(\n",
    "        dataset=f_mnist_validation, batch_size=wandb.config.batch_size,\n",
    "        pin_memory=True, num_workers=num_data_loading_workers\n",
    "    )\n",
    "\n",
    "    f_mnist_transforms = nn.Sequential(\n",
    "        transforms.ConvertImageDtype(torch.float),\n",
    "        transforms.Normalize(mean=0.28604060411453247, std=0.3530242443084717),\n",
    "        #추가\n",
    "        nn.Flatten(),\n",
    "    )\n",
    "\n",
    "    return train_data_loader, validation_data_loader, f_mnist_transforms\n",
    "\n",
    "\n",
    "def get_fashion_mnist_test_data():\n",
    "    data_path = os.path.join(BASE_PATH, \"_00_data\", \"j_fashion_mnist\")\n",
    "\n",
    "    f_mnist_test_images = datasets.FashionMNIST(data_path, train=False, download=True)\n",
    "    f_mnist_test = datasets.FashionMNIST(data_path, train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "    print(\"Num Test Samples: \", len(f_mnist_test))\n",
    "    print(\"Sample Shape: \", f_mnist_test[0][0].shape)  # torch.Size([1, 28, 28])\n",
    "\n",
    "    test_data_loader = DataLoader(dataset=f_mnist_test, batch_size=len(f_mnist_test))\n",
    "\n",
    "    f_mnist_transforms = nn.Sequential(\n",
    "        transforms.ConvertImageDtype(torch.float),\n",
    "        transforms.Normalize(mean=0.28604060411453247, std=0.3530242443084717),\n",
    "        #추가\n",
    "        nn.Flatten(),\n",
    "    )    \n",
    "\n",
    "    return f_mnist_test_images, test_data_loader, f_mnist_transforms\n",
    "\n",
    "\n",
    "\n",
    "def train_and_evaluate(epochs, train_data_loader, validation_data_loader):\n",
    "    # 수정됨: EarlyStopping 클래스 추가\n",
    "    from datetime import datetime\n",
    "    early_stopping = EarlyStopping(\n",
    "        patience=wandb.config.early_stop_patience,  # WandB에서 patience 설정\n",
    "        delta=wandb.config.early_stop_delta,       # WandB에서 delta 설정\n",
    "        project_name=\"fashion_mnist\",\n",
    "        checkpoint_file_path=str(BASE_PATH / \"checkpoints\"),  # 모델 체크포인트 저장 경로\n",
    "        run_time_str=datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Define a simple CNN model\n",
    "    model = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(28 * 28, 128),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(128, 10),\n",
    "        nn.Softmax(dim=1)  # Softmax 추가\n",
    "    ).to(device)\n",
    "\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=wandb.config.learning_rate)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=wandb.config.learning_rate, weight_decay=0.02)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # 수정됨: Training 시작 시간 기록\n",
    "    training_start_time = datetime.now()\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss, train_correct, train_total = 0, 0, 0  # Train loss and accuracy tracking 추가\n",
    "        for images, labels in train_data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()  # Loss 누적\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()  # 정확도 계산\n",
    "\n",
    "        train_accuracy = train_correct / train_total  # Train accuracy 계산\n",
    "        train_loss /= len(train_data_loader)  # 평균 Train loss 계산\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0, 0, 0  # Validation loss and accuracy tracking 추가\n",
    "        with torch.no_grad():\n",
    "            for images, labels in validation_data_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()  # Validation loss 누적\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()  # 정확도 계산\n",
    "\n",
    "        val_accuracy = val_correct / val_total  # Validation accuracy 계산\n",
    "        val_loss /= len(validation_data_loader)  # 평균 Validation loss 계산\n",
    "\n",
    "        # 수정됨: EarlyStopping 체크 추가\n",
    "        message, early_stop = early_stopping.check_and_save(val_loss, model)\n",
    "\n",
    "        # Log metrics to wandb\n",
    "        wandb.log({  # Train/Validation loss와 accuracy를 wandb에 기록 추가\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_accuracy\": train_accuracy,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_accuracy\": val_accuracy\n",
    "        })\n",
    "\n",
    "        # Epoch별 결과 출력 추가\n",
    "        elapsed_time = datetime.now() - training_start_time\n",
    "        print(f\"Epoch [{epoch}/{epochs}] - Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f} | {message}\")\n",
    "\n",
    "        # 수정됨: Early stopping 조건 충족 시 종료\n",
    "        if early_stop:\n",
    "            print(\"Early stopping triggered. Stopping training.\")\n",
    "            break\n",
    "\n",
    "    # 수정됨: 최종 훈련 시간 출력\n",
    "    elapsed_time = datetime.now() - training_start_time\n",
    "    print(f\"Final training time: {elapsed_time}\")\n",
    "\n",
    "# 외부 코드 추가\n",
    "from torch.optim import Adam\n",
    "\n",
    "# 외부 코드 추가\n",
    "# Define CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(64 * 14 * 14, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = {\n",
    "    \"batch_size\": 128,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"epochs\": 20,\n",
    "    \"early_stop_patience\": 5,  # EarlyStopping patience 설정\n",
    "    \"early_stop_delta\": 0.001  # EarlyStopping delta 설정\n",
    "}\n",
    "\n",
    "    wandb.init(mode=\"disabled\", config=config)\n",
    "\n",
    "    train_data_loader, validation_data_loader, f_mnist_transforms = get_fashion_mnist_data()\n",
    "    print()\n",
    "    f_mnist_test_images, test_data_loader, f_mnist_transforms = get_fashion_mnist_test_data()\n",
    "# 외부 코드 추가\n",
    "    # Model, Optimizer, Criterion\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = SimpleCNN().to(device)\n",
    "#     외부 코드 원본\n",
    "#     optimizer = Adam(model.parameters(), lr=config.learning_rate)\n",
    "    optimizer = Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "# 추가\n",
    "    # Train and Validate\n",
    "#    외부 코드 원본 \n",
    "#     train_and_validate(model, train_loader, validation_loader, config.epochs, device, optimizer, criterion)\n",
    "#     train_and_evaluate(model, train_data_loader, validation_data_loader, config[\"epochs\"])\n",
    "\n",
    "    # Start training and evaluating\n",
    "    # 원래 원본\n",
    "    train_and_evaluate(wandb.config.epochs, train_data_loader, validation_data_loader)\n",
    "\n",
    "    wandb.finish()\n",
    "    \n",
    "    \n",
    "# 외부 코드\n",
    "# Get Data Loaders\n",
    "#     train_loader, validation_loader = get_fashion_mnist_data(config.batch_size)\n",
    "\n",
    "#     # Model, Optimizer, Criterion\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     model = SimpleCNN().to(device)\n",
    "#     optimizer = Adam(model.parameters(), lr=config.learning_rate)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#     # Train and Validate\n",
    "#     train_and_validate(model, train_loader, validation_loader, config.epochs, device, optimizer, criterion)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed18f439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 코드에서 Training loss, validation loss, Training Accuracy, Validation Accuracy 넣기.version 2\n",
    "# dropout 추가함\n",
    "# early patience 추가\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "from early_stopping import EarlyStopping  \n",
    "\n",
    "# Use the current working directory\n",
    "BASE_PATH = Path().resolve()\n",
    "print(BASE_PATH)\n",
    "\n",
    "# 체크포인트 디렉터리 생성\n",
    "os.makedirs(BASE_PATH / \"checkpoints\", exist_ok=True)\n",
    "\n",
    "import sys\n",
    "sys.path.append(str(BASE_PATH))\n",
    "\n",
    "from utils import get_num_cpu_cores, is_linux, is_windows\n",
    "\n",
    "\n",
    "\n",
    "def get_fashion_mnist_data():\n",
    "    data_path = os.path.join(BASE_PATH, \"_00_data\", \"j_fashion_mnist\")\n",
    "\n",
    "    f_mnist_train = datasets.FashionMNIST(data_path, train=True, download=True, transform=transforms.ToTensor())\n",
    "    f_mnist_train, f_mnist_validation = random_split(f_mnist_train, [55_000, 5_000])\n",
    "\n",
    "    print(\"Num Train Samples: \", len(f_mnist_train))\n",
    "    print(\"Num Validation Samples: \", len(f_mnist_validation))\n",
    "    print(\"Sample Shape: \", f_mnist_train[0][0].shape)  # torch.Size([1, 28, 28])\n",
    "\n",
    "    num_data_loading_workers = get_num_cpu_cores() if is_linux() or is_windows() else 0\n",
    "    print(\"Number of Data Loading Workers:\", num_data_loading_workers)\n",
    "\n",
    "    train_data_loader = DataLoader(\n",
    "        dataset=f_mnist_train, batch_size=wandb.config.batch_size, shuffle=True,\n",
    "        pin_memory=True, num_workers=num_data_loading_workers\n",
    "    )\n",
    "\n",
    "    validation_data_loader = DataLoader(\n",
    "        dataset=f_mnist_validation, batch_size=wandb.config.batch_size,\n",
    "        pin_memory=True, num_workers=num_data_loading_workers\n",
    "    )\n",
    "\n",
    "    f_mnist_transforms = nn.Sequential(\n",
    "        transforms.ConvertImageDtype(torch.float),\n",
    "        transforms.Normalize(mean=0.286, std=0.353),\n",
    "        #추가\n",
    "        nn.Flatten(),\n",
    "    )\n",
    "\n",
    "    return train_data_loader, validation_data_loader, f_mnist_transforms\n",
    "\n",
    "\n",
    "def get_fashion_mnist_test_data():\n",
    "    data_path = os.path.join(BASE_PATH, \"_00_data\", \"j_fashion_mnist\")\n",
    "\n",
    "    f_mnist_test_images = datasets.FashionMNIST(data_path, train=False, download=True)\n",
    "    f_mnist_test = datasets.FashionMNIST(data_path, train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "    print(\"Num Test Samples: \", len(f_mnist_test))\n",
    "    print(\"Sample Shape: \", f_mnist_test[0][0].shape)  # torch.Size([1, 28, 28])\n",
    "\n",
    "    test_data_loader = DataLoader(dataset=f_mnist_test, batch_size=len(f_mnist_test))\n",
    "\n",
    "    f_mnist_transforms = nn.Sequential(\n",
    "        transforms.ConvertImageDtype(torch.float),\n",
    "        transforms.Normalize(mean=0.286, std=0.353),\n",
    "        #추가\n",
    "        nn.Flatten(),\n",
    "    )    \n",
    "\n",
    "    return f_mnist_test_images, test_data_loader, f_mnist_transforms\n",
    "\n",
    "\n",
    "\n",
    "def train_and_evaluate(epochs, train_data_loader, validation_data_loader):\n",
    "    # 수정됨: EarlyStopping 클래스 추가\n",
    "    from datetime import datetime\n",
    "    early_stopping = EarlyStopping(\n",
    "        patience=wandb.config.early_stop_patience,  # WandB에서 patience 설정\n",
    "        delta=wandb.config.early_stop_delta,       # WandB에서 delta 설정\n",
    "        project_name=\"fashion_mnist\",\n",
    "        checkpoint_file_path=str(BASE_PATH / \"checkpoints\"),  # 모델 체크포인트 저장 경로\n",
    "        run_time_str=datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Define a simple CNN model\n",
    "    model = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(28 * 28, 128),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.Linear(128, 10),\n",
    "        nn.Softmax(dim=1)  # Softmax 추가\n",
    "    ).to(device)\n",
    "\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=wandb.config.learning_rate)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=wandb.config.learning_rate, weight_decay=0.02)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # 수정됨: Training 시작 시간 기록\n",
    "    training_start_time = datetime.now()\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss, train_correct, train_total = 0, 0, 0  # Train loss and accuracy tracking 추가\n",
    "        for images, labels in train_data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()  # Loss 누적\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()  # 정확도 계산\n",
    "\n",
    "        train_accuracy = train_correct / train_total  # Train accuracy 계산\n",
    "        train_loss /= len(train_data_loader)  # 평균 Train loss 계산\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0, 0, 0  # Validation loss and accuracy tracking 추가\n",
    "        with torch.no_grad():\n",
    "            for images, labels in validation_data_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()  # Validation loss 누적\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()  # 정확도 계산\n",
    "\n",
    "        val_accuracy = val_correct / val_total  # Validation accuracy 계산\n",
    "        val_loss /= len(validation_data_loader)  # 평균 Validation loss 계산\n",
    "\n",
    "        # 수정됨: EarlyStopping 체크 추가\n",
    "        message, early_stop = early_stopping.check_and_save(val_loss, model)\n",
    "\n",
    "        # Log metrics to wandb\n",
    "        wandb.log({  # Train/Validation loss와 accuracy를 wandb에 기록 추가\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_accuracy\": train_accuracy,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_accuracy\": val_accuracy\n",
    "        })\n",
    "\n",
    "        # Epoch별 결과 출력 추가\n",
    "        elapsed_time = datetime.now() - training_start_time\n",
    "        print(f\"Epoch [{epoch}/{epochs}] - Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f} | {message}\")\n",
    "\n",
    "        # 수정됨: Early stopping 조건 충족 시 종료\n",
    "        if early_stop:\n",
    "            print(\"Early stopping triggered. Stopping training.\")\n",
    "            break\n",
    "\n",
    "    # 수정됨: 최종 훈련 시간 출력\n",
    "    elapsed_time = datetime.now() - training_start_time\n",
    "    print(f\"Final training time: {elapsed_time}\")\n",
    "\n",
    "# 외부 코드 추가\n",
    "from torch.optim import Adam\n",
    "\n",
    "# 외부 코드 추가\n",
    "# Define CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(64 * 14 * 14, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = {\n",
    "        \"batch_size\": 128,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"epochs\": 20,\n",
    "        \"early_stop_patience\": 5,\n",
    "        \"early_stop_delta\": 0.001,\n",
    "        \"weight_decay\": 0.02  # Weight Decay 설정 추가\n",
    "    }\n",
    "\n",
    "    wandb.init(mode=\"disabled\", config=config)\n",
    "\n",
    "    train_data_loader, validation_data_loader, f_mnist_transforms = get_fashion_mnist_data()\n",
    "    print()\n",
    "    f_mnist_test_images, test_data_loader, f_mnist_transforms = get_fashion_mnist_test_data()\n",
    "# 외부 코드 추가\n",
    "    # Model, Optimizer, Criterion\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = SimpleCNN().to(device)\n",
    "#     외부 코드 원본\n",
    "#     optimizer = Adam(model.parameters(), lr=config.learning_rate)\n",
    "    optimizer = Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "# 추가\n",
    "    # Train and Validate\n",
    "#    외부 코드 원본 \n",
    "#     train_and_validate(model, train_loader, validation_loader, config.epochs, device, optimizer, criterion)\n",
    "#     train_and_evaluate(model, train_data_loader, validation_data_loader, config[\"epochs\"])\n",
    "\n",
    "    # Start training and evaluating\n",
    "    # 원래 원본\n",
    "    train_and_evaluate(wandb.config.epochs, train_data_loader, validation_data_loader)\n",
    "\n",
    "    wandb.finish()\n",
    "    \n",
    "    \n",
    "# 외부 코드\n",
    "# Get Data Loaders\n",
    "#     train_loader, validation_loader = get_fashion_mnist_data(config.batch_size)\n",
    "\n",
    "#     # Model, Optimizer, Criterion\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     model = SimpleCNN().to(device)\n",
    "#     optimizer = Adam(model.parameters(), lr=config.learning_rate)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#     # Train and Validate\n",
    "#     train_and_validate(model, train_loader, validation_loader, config.epochs, device, optimizer, criterion)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "655eb588",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/work/DL\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/work/DL/wandb/run-20241121_042944-wq8kvud1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST/runs/wq8kvud1' target=\"_blank\">swift-galaxy-10</a></strong> to <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST/runs/wq8kvud1' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST/runs/wq8kvud1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Train Samples:  55000\n",
      "Num Validation Samples:  5000\n",
      "Sample Shape:  torch.Size([1, 28, 28])\n",
      "Epoch [1/20], Train Loss: 0.4053, Train Accuracy: 0.8569, Validation Loss: 0.3223, Validation Accuracy: 0.8794\n",
      "Epoch [2/20], Train Loss: 0.2484, Train Accuracy: 0.9098, Validation Loss: 0.2825, Validation Accuracy: 0.8974\n",
      "Epoch [3/20], Train Loss: 0.1981, Train Accuracy: 0.9260, Validation Loss: 0.2621, Validation Accuracy: 0.9094\n",
      "Epoch [4/20], Train Loss: 0.1618, Train Accuracy: 0.9393, Validation Loss: 0.2523, Validation Accuracy: 0.9134\n",
      "Epoch [5/20], Train Loss: 0.1270, Train Accuracy: 0.9536, Validation Loss: 0.2553, Validation Accuracy: 0.9150\n",
      "Epoch [6/20], Train Loss: 0.1002, Train Accuracy: 0.9627, Validation Loss: 0.2850, Validation Accuracy: 0.9192\n",
      "Epoch [7/20], Train Loss: 0.0770, Train Accuracy: 0.9717, Validation Loss: 0.2924, Validation Accuracy: 0.9174\n",
      "Epoch [8/20], Train Loss: 0.0578, Train Accuracy: 0.9788, Validation Loss: 0.3230, Validation Accuracy: 0.9160\n",
      "Epoch [9/20], Train Loss: 0.0438, Train Accuracy: 0.9841, Validation Loss: 0.3562, Validation Accuracy: 0.9212\n",
      "Epoch [10/20], Train Loss: 0.0334, Train Accuracy: 0.9882, Validation Loss: 0.4566, Validation Accuracy: 0.9134\n",
      "Epoch [11/20], Train Loss: 0.0255, Train Accuracy: 0.9910, Validation Loss: 0.4267, Validation Accuracy: 0.9206\n",
      "Epoch [12/20], Train Loss: 0.0222, Train Accuracy: 0.9926, Validation Loss: 0.4502, Validation Accuracy: 0.9180\n",
      "Epoch [13/20], Train Loss: 0.0213, Train Accuracy: 0.9931, Validation Loss: 0.4736, Validation Accuracy: 0.9148\n",
      "Epoch [14/20], Train Loss: 0.0161, Train Accuracy: 0.9945, Validation Loss: 0.5514, Validation Accuracy: 0.9174\n",
      "Epoch [15/20], Train Loss: 0.0179, Train Accuracy: 0.9937, Validation Loss: 0.5223, Validation Accuracy: 0.9098\n",
      "Epoch [16/20], Train Loss: 0.0148, Train Accuracy: 0.9948, Validation Loss: 0.5611, Validation Accuracy: 0.9180\n",
      "Epoch [17/20], Train Loss: 0.0151, Train Accuracy: 0.9946, Validation Loss: 0.5676, Validation Accuracy: 0.9182\n",
      "Epoch [18/20], Train Loss: 0.0109, Train Accuracy: 0.9964, Validation Loss: 0.5830, Validation Accuracy: 0.9192\n",
      "Epoch [19/20], Train Loss: 0.0095, Train Accuracy: 0.9971, Validation Loss: 0.6405, Validation Accuracy: 0.9194\n",
      "Epoch [20/20], Train Loss: 0.0137, Train Accuracy: 0.9954, Validation Loss: 0.6237, Validation Accuracy: 0.9140\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▄▄▅▆▆▇▇▇███████████</td></tr><tr><td>Train Loss</td><td>█▅▄▄▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation Accuracy</td><td>▁▄▆▇▇█▇▇█▇█▇▇▇▆▇▇██▇</td></tr><tr><td>Validation Loss</td><td>▂▂▁▁▁▂▂▂▃▅▄▅▅▆▆▇▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>20</td></tr><tr><td>Train Accuracy</td><td>0.99544</td></tr><tr><td>Train Loss</td><td>0.01367</td></tr><tr><td>Validation Accuracy</td><td>0.914</td></tr><tr><td>Validation Loss</td><td>0.62371</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">swift-galaxy-10</strong> at: <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST/runs/wq8kvud1' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST/runs/wq8kvud1</a><br/> View project at: <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241121_042944-wq8kvud1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Define CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(64 * 14 * 14, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # WandB Init\n",
    "    wandb.init(\n",
    "        project=\"FashionMNIST\",\n",
    "        config={\n",
    "            \"batch_size\": 64,\n",
    "            \"epochs\": 20,\n",
    "            \"learning_rate\": 0.001,\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "09056c51",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/work/DL\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/work/DL/wandb/run-20241121_044331-7ugzpdq6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST/runs/7ugzpdq6' target=\"_blank\">wise-lake-12</a></strong> to <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST/runs/7ugzpdq6' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST/runs/7ugzpdq6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Train Samples:  55000\n",
      "Num Validation Samples:  5000\n",
      "Sample Shape:  torch.Size([1, 28, 28])\n",
      "Epoch [1/20], Train Loss: 0.4489, Train Accuracy: 0.8387, Validation Loss: 0.3257, Validation Accuracy: 0.8888\n",
      "Epoch [2/20], Train Loss: 0.2728, Train Accuracy: 0.9017, Validation Loss: 0.2705, Validation Accuracy: 0.9078\n",
      "Epoch [3/20], Train Loss: 0.2218, Train Accuracy: 0.9185, Validation Loss: 0.2597, Validation Accuracy: 0.9108\n",
      "Epoch [4/20], Train Loss: 0.1893, Train Accuracy: 0.9298, Validation Loss: 0.2440, Validation Accuracy: 0.9184\n",
      "Epoch [5/20], Train Loss: 0.1635, Train Accuracy: 0.9395, Validation Loss: 0.2393, Validation Accuracy: 0.9200\n",
      "Epoch [6/20], Train Loss: 0.1389, Train Accuracy: 0.9491, Validation Loss: 0.2283, Validation Accuracy: 0.9208\n",
      "Epoch [7/20], Train Loss: 0.1144, Train Accuracy: 0.9588, Validation Loss: 0.2315, Validation Accuracy: 0.9254\n",
      "Epoch [8/20], Train Loss: 0.0980, Train Accuracy: 0.9644, Validation Loss: 0.2485, Validation Accuracy: 0.9274\n",
      "Epoch [9/20], Train Loss: 0.0780, Train Accuracy: 0.9711, Validation Loss: 0.2538, Validation Accuracy: 0.9218\n",
      "Epoch [10/20], Train Loss: 0.0640, Train Accuracy: 0.9767, Validation Loss: 0.2854, Validation Accuracy: 0.9248\n",
      "Epoch [11/20], Train Loss: 0.0511, Train Accuracy: 0.9820, Validation Loss: 0.2955, Validation Accuracy: 0.9216\n",
      "Epoch [12/20], Train Loss: 0.0430, Train Accuracy: 0.9850, Validation Loss: 0.3023, Validation Accuracy: 0.9200\n",
      "Epoch [13/20], Train Loss: 0.0329, Train Accuracy: 0.9890, Validation Loss: 0.3338, Validation Accuracy: 0.9260\n",
      "Epoch [14/20], Train Loss: 0.0263, Train Accuracy: 0.9907, Validation Loss: 0.3938, Validation Accuracy: 0.9188\n",
      "Epoch [15/20], Train Loss: 0.0237, Train Accuracy: 0.9918, Validation Loss: 0.3629, Validation Accuracy: 0.9242\n",
      "Epoch [16/20], Train Loss: 0.0222, Train Accuracy: 0.9924, Validation Loss: 0.3689, Validation Accuracy: 0.9234\n",
      "Epoch [17/20], Train Loss: 0.0147, Train Accuracy: 0.9954, Validation Loss: 0.4395, Validation Accuracy: 0.9196\n",
      "Epoch [18/20], Train Loss: 0.0191, Train Accuracy: 0.9935, Validation Loss: 0.4471, Validation Accuracy: 0.9230\n",
      "Epoch [19/20], Train Loss: 0.0192, Train Accuracy: 0.9934, Validation Loss: 0.4657, Validation Accuracy: 0.9218\n",
      "Epoch [20/20], Train Loss: 0.0126, Train Accuracy: 0.9959, Validation Loss: 0.4508, Validation Accuracy: 0.9214\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▄▅▅▅▆▆▇▇▇▇█████████</td></tr><tr><td>Train Loss</td><td>█▅▄▄▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation Accuracy</td><td>▁▄▅▆▇▇██▇█▇▇█▆▇▇▇▇▇▇</td></tr><tr><td>Validation Loss</td><td>▄▂▂▁▁▁▁▂▂▃▃▃▄▆▅▅▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>20</td></tr><tr><td>Train Accuracy</td><td>0.99595</td></tr><tr><td>Train Loss</td><td>0.01259</td></tr><tr><td>Validation Accuracy</td><td>0.9214</td></tr><tr><td>Validation Loss</td><td>0.45084</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">wise-lake-12</strong> at: <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST/runs/7ugzpdq6' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST/runs/7ugzpdq6</a><br/> View project at: <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241121_044331-7ugzpdq6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dropna 적용하려고 함. 좋은 생각 X\n",
    "\n",
    "# Define CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(64 * 14 * 14, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # WandB Init\n",
    "    wandb.init(\n",
    "        project=\"FashionMNIST\",\n",
    "        config={\n",
    "            \"batch_size\": 128,\n",
    "            \"epochs\": 20,\n",
    "            \"learning_rate\": 0.001,\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb621c60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/work/DL\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/work/DL/wandb/run-20241121_024830-bs0uhpte</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST/runs/bs0uhpte' target=\"_blank\">still-jazz-4</a></strong> to <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST/runs/bs0uhpte' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST/runs/bs0uhpte</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Train Samples:  55000\n",
      "Num Validation Samples:  5000\n",
      "Sample Shape:  torch.Size([1, 28, 28])\n",
      "Epoch [1/20], Train Loss: 0.4506, Train Accuracy: 0.8385, Validation Loss: 0.2988, Validation Accuracy: 0.8894\n",
      "Epoch [2/20], Train Loss: 0.2656, Train Accuracy: 0.9042, Validation Loss: 0.2633, Validation Accuracy: 0.9070\n",
      "Epoch [3/20], Train Loss: 0.2161, Train Accuracy: 0.9218, Validation Loss: 0.2145, Validation Accuracy: 0.9204\n",
      "Epoch [4/20], Train Loss: 0.1774, Train Accuracy: 0.9361, Validation Loss: 0.2168, Validation Accuracy: 0.9202\n",
      "Epoch [5/20], Train Loss: 0.1475, Train Accuracy: 0.9465, Validation Loss: 0.2062, Validation Accuracy: 0.9260\n",
      "Epoch [6/20], Train Loss: 0.1227, Train Accuracy: 0.9550, Validation Loss: 0.2202, Validation Accuracy: 0.9226\n",
      "Epoch [7/20], Train Loss: 0.0985, Train Accuracy: 0.9645, Validation Loss: 0.2217, Validation Accuracy: 0.9234\n",
      "Epoch [8/20], Train Loss: 0.0751, Train Accuracy: 0.9729, Validation Loss: 0.2315, Validation Accuracy: 0.9252\n",
      "Epoch [9/20], Train Loss: 0.0606, Train Accuracy: 0.9780, Validation Loss: 0.2565, Validation Accuracy: 0.9272\n",
      "Epoch [10/20], Train Loss: 0.0455, Train Accuracy: 0.9837, Validation Loss: 0.2808, Validation Accuracy: 0.9234\n",
      "Epoch [11/20], Train Loss: 0.0322, Train Accuracy: 0.9894, Validation Loss: 0.2995, Validation Accuracy: 0.9274\n",
      "Epoch [12/20], Train Loss: 0.0298, Train Accuracy: 0.9896, Validation Loss: 0.3164, Validation Accuracy: 0.9222\n",
      "Epoch [13/20], Train Loss: 0.0225, Train Accuracy: 0.9924, Validation Loss: 0.3358, Validation Accuracy: 0.9278\n",
      "Epoch [14/20], Train Loss: 0.0160, Train Accuracy: 0.9949, Validation Loss: 0.3562, Validation Accuracy: 0.9260\n",
      "Epoch [15/20], Train Loss: 0.0164, Train Accuracy: 0.9951, Validation Loss: 0.4161, Validation Accuracy: 0.9242\n",
      "Epoch [16/20], Train Loss: 0.0154, Train Accuracy: 0.9951, Validation Loss: 0.3784, Validation Accuracy: 0.9292\n",
      "Epoch [17/20], Train Loss: 0.0150, Train Accuracy: 0.9946, Validation Loss: 0.3722, Validation Accuracy: 0.9302\n",
      "Epoch [18/20], Train Loss: 0.0146, Train Accuracy: 0.9953, Validation Loss: 0.3934, Validation Accuracy: 0.9260\n",
      "Epoch [19/20], Train Loss: 0.0131, Train Accuracy: 0.9955, Validation Loss: 0.4734, Validation Accuracy: 0.9268\n",
      "Epoch [20/20], Train Loss: 0.0076, Train Accuracy: 0.9974, Validation Loss: 0.4864, Validation Accuracy: 0.9238\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▄▅▅▆▆▇▇▇▇██████████</td></tr><tr><td>Train Loss</td><td>█▅▄▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation Accuracy</td><td>▁▄▆▆▇▇▇▇▇▇█▇█▇▇██▇▇▇</td></tr><tr><td>Validation Loss</td><td>▃▂▁▁▁▁▁▂▂▃▃▄▄▅▆▅▅▆██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>20</td></tr><tr><td>Train Accuracy</td><td>0.99738</td></tr><tr><td>Train Loss</td><td>0.0076</td></tr><tr><td>Validation Accuracy</td><td>0.9238</td></tr><tr><td>Validation Loss</td><td>0.48643</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">still-jazz-4</strong> at: <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST/runs/bs0uhpte' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST/runs/bs0uhpte</a><br/> View project at: <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241121_024830-bs0uhpte/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import wandb\n",
    "import random\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Use the current working directory\n",
    "BASE_PATH = Path().resolve()\n",
    "print(BASE_PATH)\n",
    "\n",
    "def get_fashion_mnist_data(batch_size):\n",
    "    data_path = os.path.join(BASE_PATH, \"_00_data\", \"j_fashion_mnist\")\n",
    "    f_mnist_train = datasets.FashionMNIST(data_path, train=True, download=True, transform=transforms.ToTensor())\n",
    "    f_mnist_train, f_mnist_validation = random_split(f_mnist_train, [55_000, 5_000])\n",
    "\n",
    "    print(\"Num Train Samples: \", len(f_mnist_train))\n",
    "    print(\"Num Validation Samples: \", len(f_mnist_validation))\n",
    "    print(\"Sample Shape: \", f_mnist_train[0][0].shape)  # torch.Size([1, 28, 28])\n",
    "\n",
    "    train_data_loader = DataLoader(f_mnist_train, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "    validation_data_loader = DataLoader(f_mnist_validation, batch_size=batch_size, pin_memory=True)\n",
    "\n",
    "    return train_data_loader, validation_data_loader\n",
    "\n",
    "# Define CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(64 * 14 * 14, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Training Loop\n",
    "def train_and_validate(model, train_loader, validation_loader, epochs, device, optimizer, criterion):\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_accuracy = correct / total\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in validation_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_accuracy = val_correct / val_total\n",
    "        val_loss /= len(validation_loader)\n",
    "\n",
    "        # Log metrics to wandb\n",
    "        wandb.log({\n",
    "            \"Epoch\": epoch + 1,\n",
    "            \"Train Loss\": train_loss,\n",
    "            \"Train Accuracy\": train_accuracy,\n",
    "            \"Validation Loss\": val_loss,\n",
    "            \"Validation Accuracy\": val_accuracy\n",
    "        })\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \"\n",
    "              f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # WandB Init\n",
    "    wandb.init(\n",
    "        project=\"FashionMNIST\",\n",
    "        config={\n",
    "            \"batch_size\": 128,\n",
    "            \"epochs\": 20,\n",
    "            \"learning_rate\": 0.001,\n",
    "        }\n",
    "    )\n",
    "    config = wandb.config\n",
    "\n",
    "    # Get Data Loaders\n",
    "    train_loader, validation_loader = get_fashion_mnist_data(config.batch_size)\n",
    "\n",
    "    # Model, Optimizer, Criterion\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = SimpleCNN().to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=config.learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train and Validate\n",
    "    train_and_validate(model, train_loader, validation_loader, config.epochs, device, optimizer, criterion)\n",
    "\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f5ab0de",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/work/DL\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:6w8uqh2h) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁█</td></tr><tr><td>Train Accuracy</td><td>▁█</td></tr><tr><td>Train Loss</td><td>█▁</td></tr><tr><td>Validation Accuracy</td><td>▁█</td></tr><tr><td>Validation Loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>2</td></tr><tr><td>Train Accuracy</td><td>0.90318</td></tr><tr><td>Train Loss</td><td>0.26858</td></tr><tr><td>Validation Accuracy</td><td>0.9126</td></tr><tr><td>Validation Loss</td><td>0.23857</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">skilled-bee-7</strong> at: <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST/runs/6w8uqh2h' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST/runs/6w8uqh2h</a><br/> View project at: <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241121_042416-6w8uqh2h/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:6w8uqh2h). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/work/DL/wandb/run-20241121_042445-3ejbxggy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST/runs/3ejbxggy' target=\"_blank\">crimson-forest-8</a></strong> to <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST/runs/3ejbxggy' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST/runs/3ejbxggy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Train Samples:  55000\n",
      "Num Validation Samples:  5000\n",
      "Sample Shape:  torch.Size([1, 28, 28])\n",
      "Epoch [1/15], Train Loss: 0.5010, Train Accuracy: 0.8235, Validation Loss: 0.3342, Validation Accuracy: 0.8790\n",
      "Epoch [2/15], Train Loss: 0.2972, Train Accuracy: 0.8934, Validation Loss: 0.2513, Validation Accuracy: 0.9124\n",
      "Epoch [3/15], Train Loss: 0.2475, Train Accuracy: 0.9099, Validation Loss: 0.2425, Validation Accuracy: 0.9104\n",
      "Epoch [4/15], Train Loss: 0.2150, Train Accuracy: 0.9208, Validation Loss: 0.2312, Validation Accuracy: 0.9154\n",
      "Epoch [5/15], Train Loss: 0.1900, Train Accuracy: 0.9304, Validation Loss: 0.2363, Validation Accuracy: 0.9150\n",
      "Epoch [6/15], Train Loss: 0.1659, Train Accuracy: 0.9383, Validation Loss: 0.2114, Validation Accuracy: 0.9250\n",
      "Epoch [7/15], Train Loss: 0.1445, Train Accuracy: 0.9481, Validation Loss: 0.2124, Validation Accuracy: 0.9240\n",
      "Epoch [8/15], Train Loss: 0.1272, Train Accuracy: 0.9534, Validation Loss: 0.2108, Validation Accuracy: 0.9270\n",
      "Epoch [9/15], Train Loss: 0.1101, Train Accuracy: 0.9595, Validation Loss: 0.2180, Validation Accuracy: 0.9286\n",
      "Epoch [10/15], Train Loss: 0.0943, Train Accuracy: 0.9648, Validation Loss: 0.2087, Validation Accuracy: 0.9290\n",
      "Epoch [11/15], Train Loss: 0.0778, Train Accuracy: 0.9723, Validation Loss: 0.2221, Validation Accuracy: 0.9296\n",
      "Epoch [12/15], Train Loss: 0.0651, Train Accuracy: 0.9766, Validation Loss: 0.2340, Validation Accuracy: 0.9318\n",
      "Epoch [13/15], Train Loss: 0.0509, Train Accuracy: 0.9828, Validation Loss: 0.2548, Validation Accuracy: 0.9266\n",
      "Epoch [14/15], Train Loss: 0.0449, Train Accuracy: 0.9842, Validation Loss: 0.2992, Validation Accuracy: 0.9208\n",
      "Epoch [15/15], Train Loss: 0.0375, Train Accuracy: 0.9871, Validation Loss: 0.2865, Validation Accuracy: 0.9308\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>Train Accuracy</td><td>▁▄▅▅▆▆▆▇▇▇▇████</td></tr><tr><td>Train Loss</td><td>█▅▄▄▃▃▃▂▂▂▂▁▁▁▁</td></tr><tr><td>Validation Accuracy</td><td>▁▅▅▆▆▇▇▇████▇▇█</td></tr><tr><td>Validation Loss</td><td>█▃▃▂▃▁▁▁▂▁▂▂▄▆▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>15</td></tr><tr><td>Train Accuracy</td><td>0.98715</td></tr><tr><td>Train Loss</td><td>0.03751</td></tr><tr><td>Validation Accuracy</td><td>0.9308</td></tr><tr><td>Validation Loss</td><td>0.28648</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">crimson-forest-8</strong> at: <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST/runs/3ejbxggy' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST/runs/3ejbxggy</a><br/> View project at: <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241121_042445-3ejbxggy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import wandb\n",
    "import random\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Use the current working directory\n",
    "BASE_PATH = Path().resolve()\n",
    "print(BASE_PATH)\n",
    "\n",
    "def get_fashion_mnist_data(batch_size):\n",
    "    data_path = os.path.join(BASE_PATH, \"_00_data\", \"j_fashion_mnist\")\n",
    "    f_mnist_train = datasets.FashionMNIST(data_path, train=True, download=True, transform=transforms.ToTensor())\n",
    "    f_mnist_train, f_mnist_validation = random_split(f_mnist_train, [55_000, 5_000])\n",
    "\n",
    "    print(\"Num Train Samples: \", len(f_mnist_train))\n",
    "    print(\"Num Validation Samples: \", len(f_mnist_validation))\n",
    "    print(\"Sample Shape: \", f_mnist_train[0][0].shape)  # torch.Size([1, 28, 28])\n",
    "\n",
    "    train_data_loader = DataLoader(f_mnist_train, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "    validation_data_loader = DataLoader(f_mnist_validation, batch_size=batch_size, pin_memory=True)\n",
    "\n",
    "    return train_data_loader, validation_data_loader\n",
    "\n",
    "# Define CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(64 * 14 * 14, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Training Loop\n",
    "def train_and_validate(model, train_loader, validation_loader, epochs, device, optimizer, criterion):\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_accuracy = correct / total\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in validation_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_accuracy = val_correct / val_total\n",
    "        val_loss /= len(validation_loader)\n",
    "\n",
    "        # Log metrics to wandb\n",
    "        wandb.log({\n",
    "            \"Epoch\": epoch + 1,\n",
    "            \"Train Loss\": train_loss,\n",
    "            \"Train Accuracy\": train_accuracy,\n",
    "            \"Validation Loss\": val_loss,\n",
    "            \"Validation Accuracy\": val_accuracy\n",
    "        })\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \"\n",
    "              f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # WandB Init\n",
    "    wandb.init(\n",
    "        project=\"FashionMNIST\",\n",
    "        config={\n",
    "            \"batch_size\": 256,\n",
    "            \"epochs\": 15,\n",
    "            \"learning_rate\": 0.001,\n",
    "        }\n",
    "    )\n",
    "    config = wandb.config\n",
    "\n",
    "    # Get Data Loaders\n",
    "    train_loader, validation_loader = get_fashion_mnist_data(config.batch_size)\n",
    "\n",
    "    # Model, Optimizer, Criterion\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = SimpleCNN().to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=config.learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train and Validate\n",
    "    train_and_validate(model, train_loader, validation_loader, config.epochs, device, optimizer, criterion)\n",
    "\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b72f23d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/work/DL\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2502c190d5144ee84f76a873fd1b32a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113140235344569, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/work/DL/wandb/run-20241121_042643-rn8eslbf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST/runs/rn8eslbf' target=\"_blank\">legendary-oath-9</a></strong> to <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST/runs/rn8eslbf' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST/runs/rn8eslbf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Train Samples:  55000\n",
      "Num Validation Samples:  5000\n",
      "Sample Shape:  torch.Size([1, 28, 28])\n",
      "Epoch [1/20], Train Loss: 0.5074, Train Accuracy: 0.8209, Validation Loss: 0.3951, Validation Accuracy: 0.8512\n",
      "Epoch [2/20], Train Loss: 0.3016, Train Accuracy: 0.8919, Validation Loss: 0.2779, Validation Accuracy: 0.9024\n",
      "Epoch [3/20], Train Loss: 0.2514, Train Accuracy: 0.9085, Validation Loss: 0.2530, Validation Accuracy: 0.9088\n",
      "Epoch [4/20], Train Loss: 0.2173, Train Accuracy: 0.9211, Validation Loss: 0.2402, Validation Accuracy: 0.9136\n",
      "Epoch [5/20], Train Loss: 0.1906, Train Accuracy: 0.9295, Validation Loss: 0.2244, Validation Accuracy: 0.9184\n",
      "Epoch [6/20], Train Loss: 0.1662, Train Accuracy: 0.9401, Validation Loss: 0.2146, Validation Accuracy: 0.9232\n",
      "Epoch [7/20], Train Loss: 0.1446, Train Accuracy: 0.9469, Validation Loss: 0.2119, Validation Accuracy: 0.9262\n",
      "Epoch [8/20], Train Loss: 0.1260, Train Accuracy: 0.9535, Validation Loss: 0.2195, Validation Accuracy: 0.9214\n",
      "Epoch [9/20], Train Loss: 0.1078, Train Accuracy: 0.9611, Validation Loss: 0.2304, Validation Accuracy: 0.9278\n",
      "Epoch [10/20], Train Loss: 0.0949, Train Accuracy: 0.9655, Validation Loss: 0.2252, Validation Accuracy: 0.9266\n",
      "Epoch [11/20], Train Loss: 0.0762, Train Accuracy: 0.9728, Validation Loss: 0.2666, Validation Accuracy: 0.9140\n",
      "Epoch [12/20], Train Loss: 0.0612, Train Accuracy: 0.9783, Validation Loss: 0.2796, Validation Accuracy: 0.9180\n",
      "Epoch [13/20], Train Loss: 0.0557, Train Accuracy: 0.9798, Validation Loss: 0.2534, Validation Accuracy: 0.9260\n",
      "Epoch [14/20], Train Loss: 0.0421, Train Accuracy: 0.9864, Validation Loss: 0.2861, Validation Accuracy: 0.9238\n",
      "Epoch [15/20], Train Loss: 0.0319, Train Accuracy: 0.9895, Validation Loss: 0.3112, Validation Accuracy: 0.9248\n",
      "Epoch [16/20], Train Loss: 0.0262, Train Accuracy: 0.9917, Validation Loss: 0.3143, Validation Accuracy: 0.9240\n",
      "Epoch [17/20], Train Loss: 0.0244, Train Accuracy: 0.9924, Validation Loss: 0.3232, Validation Accuracy: 0.9228\n",
      "Epoch [18/20], Train Loss: 0.0151, Train Accuracy: 0.9955, Validation Loss: 0.3398, Validation Accuracy: 0.9252\n",
      "Epoch [19/20], Train Loss: 0.0160, Train Accuracy: 0.9951, Validation Loss: 0.3551, Validation Accuracy: 0.9238\n",
      "Epoch [20/20], Train Loss: 0.0200, Train Accuracy: 0.9932, Validation Loss: 0.3594, Validation Accuracy: 0.9246\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▄▅▅▅▆▆▆▇▇▇▇▇███████</td></tr><tr><td>Train Loss</td><td>█▅▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>Validation Accuracy</td><td>▁▆▆▇▇██▇██▇▇████████</td></tr><tr><td>Validation Loss</td><td>█▄▃▂▁▁▁▁▂▂▃▄▃▄▅▅▅▆▆▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>20</td></tr><tr><td>Train Accuracy</td><td>0.99322</td></tr><tr><td>Train Loss</td><td>0.01998</td></tr><tr><td>Validation Accuracy</td><td>0.9246</td></tr><tr><td>Validation Loss</td><td>0.35945</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">legendary-oath-9</strong> at: <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST/runs/rn8eslbf' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST/runs/rn8eslbf</a><br/> View project at: <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241121_042643-rn8eslbf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Define CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(64 * 14 * 14, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # WandB Init\n",
    "    wandb.init(\n",
    "        project=\"FashionMNIST\",\n",
    "        config={\n",
    "            \"batch_size\": 256,\n",
    "            \"epochs\": 20,\n",
    "            \"learning_rate\": 0.001,\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7cfe6ae",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/work/DL\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/work/DL/wandb/run-20241121_041917-xybx9ey5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST/runs/xybx9ey5' target=\"_blank\">dark-violet-5</a></strong> to <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST/runs/xybx9ey5' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST/runs/xybx9ey5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Train Samples:  55000\n",
      "Num Validation Samples:  5000\n",
      "Sample Shape:  torch.Size([1, 28, 28])\n",
      "Epoch [1/30], Train Loss: 0.4407, Train Accuracy: 0.8437, Validation Loss: 0.2876, Validation Accuracy: 0.8978\n",
      "Epoch [2/30], Train Loss: 0.2736, Train Accuracy: 0.9012, Validation Loss: 0.2606, Validation Accuracy: 0.9064\n",
      "Epoch [3/30], Train Loss: 0.2214, Train Accuracy: 0.9190, Validation Loss: 0.2265, Validation Accuracy: 0.9170\n",
      "Epoch [4/30], Train Loss: 0.1815, Train Accuracy: 0.9341, Validation Loss: 0.2362, Validation Accuracy: 0.9148\n",
      "Epoch [5/30], Train Loss: 0.1530, Train Accuracy: 0.9432, Validation Loss: 0.2130, Validation Accuracy: 0.9220\n",
      "Epoch [6/30], Train Loss: 0.1289, Train Accuracy: 0.9524, Validation Loss: 0.2095, Validation Accuracy: 0.9248\n",
      "Epoch [7/30], Train Loss: 0.1038, Train Accuracy: 0.9619, Validation Loss: 0.2203, Validation Accuracy: 0.9266\n",
      "Epoch [8/30], Train Loss: 0.0820, Train Accuracy: 0.9703, Validation Loss: 0.2242, Validation Accuracy: 0.9262\n",
      "Epoch [9/30], Train Loss: 0.0628, Train Accuracy: 0.9777, Validation Loss: 0.2335, Validation Accuracy: 0.9302\n",
      "Epoch [10/30], Train Loss: 0.0499, Train Accuracy: 0.9825, Validation Loss: 0.2579, Validation Accuracy: 0.9250\n",
      "Epoch [11/30], Train Loss: 0.0364, Train Accuracy: 0.9877, Validation Loss: 0.2844, Validation Accuracy: 0.9270\n",
      "Epoch [12/30], Train Loss: 0.0334, Train Accuracy: 0.9888, Validation Loss: 0.3200, Validation Accuracy: 0.9212\n",
      "Epoch [13/30], Train Loss: 0.0232, Train Accuracy: 0.9923, Validation Loss: 0.3286, Validation Accuracy: 0.9290\n",
      "Epoch [14/30], Train Loss: 0.0222, Train Accuracy: 0.9922, Validation Loss: 0.3505, Validation Accuracy: 0.9288\n",
      "Epoch [15/30], Train Loss: 0.0169, Train Accuracy: 0.9943, Validation Loss: 0.4187, Validation Accuracy: 0.9202\n",
      "Epoch [16/30], Train Loss: 0.0204, Train Accuracy: 0.9935, Validation Loss: 0.3870, Validation Accuracy: 0.9202\n",
      "Epoch [17/30], Train Loss: 0.0146, Train Accuracy: 0.9949, Validation Loss: 0.4188, Validation Accuracy: 0.9178\n",
      "Epoch [18/30], Train Loss: 0.0107, Train Accuracy: 0.9964, Validation Loss: 0.4126, Validation Accuracy: 0.9288\n",
      "Epoch [19/30], Train Loss: 0.0101, Train Accuracy: 0.9964, Validation Loss: 0.4299, Validation Accuracy: 0.9280\n",
      "Epoch [20/30], Train Loss: 0.0058, Train Accuracy: 0.9981, Validation Loss: 0.4476, Validation Accuracy: 0.9292\n",
      "Epoch [21/30], Train Loss: 0.0139, Train Accuracy: 0.9951, Validation Loss: 0.4561, Validation Accuracy: 0.9256\n",
      "Epoch [22/30], Train Loss: 0.0072, Train Accuracy: 0.9978, Validation Loss: 0.4709, Validation Accuracy: 0.9274\n",
      "Epoch [23/30], Train Loss: 0.0182, Train Accuracy: 0.9936, Validation Loss: 0.4552, Validation Accuracy: 0.9260\n",
      "Epoch [24/30], Train Loss: 0.0116, Train Accuracy: 0.9960, Validation Loss: 0.4558, Validation Accuracy: 0.9244\n",
      "Epoch [25/30], Train Loss: 0.0029, Train Accuracy: 0.9993, Validation Loss: 0.4668, Validation Accuracy: 0.9278\n",
      "Epoch [26/30], Train Loss: 0.0013, Train Accuracy: 0.9997, Validation Loss: 0.4885, Validation Accuracy: 0.9290\n",
      "Epoch [27/30], Train Loss: 0.0142, Train Accuracy: 0.9953, Validation Loss: 0.4812, Validation Accuracy: 0.9248\n",
      "Epoch [28/30], Train Loss: 0.0114, Train Accuracy: 0.9966, Validation Loss: 0.4935, Validation Accuracy: 0.9272\n",
      "Epoch [29/30], Train Loss: 0.0053, Train Accuracy: 0.9985, Validation Loss: 0.4905, Validation Accuracy: 0.9258\n",
      "Epoch [30/30], Train Loss: 0.0082, Train Accuracy: 0.9971, Validation Loss: 0.5132, Validation Accuracy: 0.9220\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>Train Accuracy</td><td>▁▄▄▅▅▆▆▇▇▇▇███████████████████</td></tr><tr><td>Train Loss</td><td>█▅▅▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation Accuracy</td><td>▁▃▅▅▆▇▇▇█▇▇▆██▆▆▅███▇▇▇▇▇█▇▇▇▆</td></tr><tr><td>Validation Loss</td><td>▃▂▁▂▁▁▁▁▂▂▃▄▄▄▆▅▆▆▆▆▇▇▇▇▇▇▇█▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>30</td></tr><tr><td>Train Accuracy</td><td>0.99715</td></tr><tr><td>Train Loss</td><td>0.00822</td></tr><tr><td>Validation Accuracy</td><td>0.922</td></tr><tr><td>Validation Loss</td><td>0.51319</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dark-violet-5</strong> at: <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST/runs/xybx9ey5' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST/runs/xybx9ey5</a><br/> View project at: <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241121_041917-xybx9ey5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(64 * 14 * 14, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # WandB Init\n",
    "    wandb.init(\n",
    "        project=\"FashionMNIST\",\n",
    "        config={\n",
    "            \"batch_size\": 128,\n",
    "            \"epochs\": 30,\n",
    "            \"learning_rate\": 0.001,\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "95494f78",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/work/DL\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/work/DL/wandb/run-20241121_112542-3u8m5hzz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Efficient/runs/3u8m5hzz' target=\"_blank\">pious-snow-1</a></strong> to <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Efficient' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Efficient' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Efficient</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Efficient/runs/3u8m5hzz' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Efficient/runs/3u8m5hzz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Train Samples:  55000\n",
      "Num Validation Samples:  5000\n",
      "Sample Shape:  torch.Size([1, 28, 28])\n",
      "Number of Data Loading Workers: 2\n",
      "Epoch [1/20], Train Loss: 0.4508, Train Accuracy: 0.8378, Validation Loss: 0.3153, Validation Accuracy: 0.8836\n",
      "Epoch [2/20], Train Loss: 0.2714, Train Accuracy: 0.9015, Validation Loss: 0.2578, Validation Accuracy: 0.9052\n",
      "Epoch [3/20], Train Loss: 0.2222, Train Accuracy: 0.9177, Validation Loss: 0.2316, Validation Accuracy: 0.9184\n",
      "Epoch [4/20], Train Loss: 0.1852, Train Accuracy: 0.9312, Validation Loss: 0.2072, Validation Accuracy: 0.9242\n",
      "Epoch [5/20], Train Loss: 0.1544, Train Accuracy: 0.9431, Validation Loss: 0.2151, Validation Accuracy: 0.9238\n",
      "Epoch [6/20], Train Loss: 0.1278, Train Accuracy: 0.9528, Validation Loss: 0.2105, Validation Accuracy: 0.9274\n",
      "Epoch [7/20], Train Loss: 0.1049, Train Accuracy: 0.9620, Validation Loss: 0.2168, Validation Accuracy: 0.9304\n",
      "Epoch [8/20], Train Loss: 0.0831, Train Accuracy: 0.9698, Validation Loss: 0.2359, Validation Accuracy: 0.9286\n",
      "Epoch [9/20], Train Loss: 0.0644, Train Accuracy: 0.9776, Validation Loss: 0.2575, Validation Accuracy: 0.9230\n",
      "Epoch [10/20], Train Loss: 0.0526, Train Accuracy: 0.9815, Validation Loss: 0.2640, Validation Accuracy: 0.9250\n",
      "Epoch [11/20], Train Loss: 0.0429, Train Accuracy: 0.9852, Validation Loss: 0.2895, Validation Accuracy: 0.9264\n",
      "Epoch [12/20], Train Loss: 0.0304, Train Accuracy: 0.9894, Validation Loss: 0.3031, Validation Accuracy: 0.9282\n",
      "Epoch [13/20], Train Loss: 0.0261, Train Accuracy: 0.9911, Validation Loss: 0.3561, Validation Accuracy: 0.9234\n",
      "Epoch [14/20], Train Loss: 0.0215, Train Accuracy: 0.9928, Validation Loss: 0.3282, Validation Accuracy: 0.9294\n",
      "Epoch [15/20], Train Loss: 0.0214, Train Accuracy: 0.9923, Validation Loss: 0.3427, Validation Accuracy: 0.9202\n",
      "Epoch [16/20], Train Loss: 0.0172, Train Accuracy: 0.9940, Validation Loss: 0.3921, Validation Accuracy: 0.9228\n",
      "Epoch [17/20], Train Loss: 0.0155, Train Accuracy: 0.9944, Validation Loss: 0.4149, Validation Accuracy: 0.9204\n",
      "Epoch [18/20], Train Loss: 0.0167, Train Accuracy: 0.9946, Validation Loss: 0.4043, Validation Accuracy: 0.9254\n",
      "Epoch [19/20], Train Loss: 0.0122, Train Accuracy: 0.9957, Validation Loss: 0.3921, Validation Accuracy: 0.9268\n",
      "Epoch [20/20], Train Loss: 0.0152, Train Accuracy: 0.9950, Validation Loss: 0.3883, Validation Accuracy: 0.9290\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>train_accuracy</td><td>▁▄▅▅▆▆▇▇▇▇██████████</td></tr><tr><td>train_loss</td><td>█▅▄▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▆▇▇███▇▇▇█▇█▆▇▇▇▇█</td></tr><tr><td>val_loss</td><td>▅▃▂▁▁▁▁▂▃▃▄▄▆▅▆▇██▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>train_accuracy</td><td>0.995</td></tr><tr><td>train_loss</td><td>0.01523</td></tr><tr><td>val_accuracy</td><td>0.929</td></tr><tr><td>val_loss</td><td>0.38825</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">pious-snow-1</strong> at: <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Efficient/runs/3u8m5hzz' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Efficient/runs/3u8m5hzz</a><br/> View project at: <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Efficient' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Efficient</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241121_112542-3u8m5hzz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(64 * 14 * 14, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize WandB\n",
    "    wandb.init(\n",
    "        project=\"FashionMNIST_Efficient\",\n",
    "        config={\n",
    "            \"learning_rate\": 0.001,\n",
    "            \"batch_size\": 128,\n",
    "            \"epochs\": 20,\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4aae3d5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/work/DL\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/work/DL/wandb/run-20241121_112846-ur5kdq8q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout/runs/ur5kdq8q' target=\"_blank\">prime-armadillo-1</a></strong> to <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout/runs/ur5kdq8q' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout/runs/ur5kdq8q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Train Samples:  55000\n",
      "Num Validation Samples:  5000\n",
      "Sample Shape:  torch.Size([1, 28, 28])\n",
      "Number of Data Loading Workers: 2\n",
      "Epoch [1/20], Train Loss: 0.5613, Train Accuracy: 0.7985, Validation Loss: 0.3276, Validation Accuracy: 0.8864\n",
      "Epoch [2/20], Train Loss: 0.3570, Train Accuracy: 0.8731, Validation Loss: 0.2656, Validation Accuracy: 0.9058\n",
      "Epoch [3/20], Train Loss: 0.3000, Train Accuracy: 0.8924, Validation Loss: 0.2527, Validation Accuracy: 0.9124\n",
      "Epoch [4/20], Train Loss: 0.2664, Train Accuracy: 0.9042, Validation Loss: 0.2311, Validation Accuracy: 0.9192\n",
      "Epoch [5/20], Train Loss: 0.2396, Train Accuracy: 0.9119, Validation Loss: 0.2224, Validation Accuracy: 0.9204\n",
      "Epoch [6/20], Train Loss: 0.2202, Train Accuracy: 0.9190, Validation Loss: 0.2109, Validation Accuracy: 0.9238\n",
      "Epoch [7/20], Train Loss: 0.1991, Train Accuracy: 0.9268, Validation Loss: 0.2012, Validation Accuracy: 0.9302\n",
      "Epoch [8/20], Train Loss: 0.1811, Train Accuracy: 0.9339, Validation Loss: 0.2030, Validation Accuracy: 0.9320\n",
      "Epoch [9/20], Train Loss: 0.1677, Train Accuracy: 0.9367, Validation Loss: 0.2115, Validation Accuracy: 0.9254\n",
      "Epoch [10/20], Train Loss: 0.1554, Train Accuracy: 0.9422, Validation Loss: 0.2127, Validation Accuracy: 0.9274\n",
      "Epoch [11/20], Train Loss: 0.1400, Train Accuracy: 0.9473, Validation Loss: 0.1971, Validation Accuracy: 0.9332\n",
      "Epoch [12/20], Train Loss: 0.1309, Train Accuracy: 0.9508, Validation Loss: 0.2131, Validation Accuracy: 0.9326\n",
      "Epoch [13/20], Train Loss: 0.1193, Train Accuracy: 0.9538, Validation Loss: 0.2146, Validation Accuracy: 0.9350\n",
      "Epoch [14/20], Train Loss: 0.1111, Train Accuracy: 0.9580, Validation Loss: 0.2200, Validation Accuracy: 0.9288\n",
      "Epoch [15/20], Train Loss: 0.1021, Train Accuracy: 0.9606, Validation Loss: 0.2164, Validation Accuracy: 0.9360\n",
      "Epoch [16/20], Train Loss: 0.0969, Train Accuracy: 0.9629, Validation Loss: 0.2379, Validation Accuracy: 0.9300\n",
      "Epoch [17/20], Train Loss: 0.0901, Train Accuracy: 0.9655, Validation Loss: 0.2394, Validation Accuracy: 0.9370\n",
      "Epoch [18/20], Train Loss: 0.0828, Train Accuracy: 0.9673, Validation Loss: 0.2279, Validation Accuracy: 0.9368\n",
      "Epoch [19/20], Train Loss: 0.0795, Train Accuracy: 0.9695, Validation Loss: 0.2421, Validation Accuracy: 0.9376\n",
      "Epoch [20/20], Train Loss: 0.0733, Train Accuracy: 0.9716, Validation Loss: 0.2563, Validation Accuracy: 0.9332\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>train_accuracy</td><td>▁▄▅▅▆▆▆▆▇▇▇▇▇▇██████</td></tr><tr><td>train_loss</td><td>█▅▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▅▅▆▆▇▇▆▇▇▇█▇█▇███▇</td></tr><tr><td>val_loss</td><td>█▅▄▃▂▂▁▁▂▂▁▂▂▂▂▃▃▃▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>train_accuracy</td><td>0.9716</td></tr><tr><td>train_loss</td><td>0.07335</td></tr><tr><td>val_accuracy</td><td>0.9332</td></tr><tr><td>val_loss</td><td>0.25626</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">prime-armadillo-1</strong> at: <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout/runs/ur5kdq8q' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout/runs/ur5kdq8q</a><br/> View project at: <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241121_112846-ur5kdq8q/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "# Use the current working directory\n",
    "BASE_PATH = Path().resolve()\n",
    "print(BASE_PATH)\n",
    "\n",
    "import sys\n",
    "sys.path.append(str(BASE_PATH))\n",
    "\n",
    "from utils import get_num_cpu_cores, is_linux, is_windows\n",
    "\n",
    "def get_fashion_mnist_data():\n",
    "    data_path = os.path.join(BASE_PATH, \"_00_data\", \"j_fashion_mnist\")\n",
    "\n",
    "    f_mnist_train = datasets.FashionMNIST(data_path, train=True, download=True, transform=transforms.ToTensor())\n",
    "    f_mnist_train, f_mnist_validation = random_split(f_mnist_train, [55_000, 5_000])\n",
    "\n",
    "    print(\"Num Train Samples: \", len(f_mnist_train))\n",
    "    print(\"Num Validation Samples: \", len(f_mnist_validation))\n",
    "    print(\"Sample Shape: \", f_mnist_train[0][0].shape)  # torch.Size([1, 28, 28])\n",
    "\n",
    "    num_data_loading_workers = get_num_cpu_cores() if is_linux() or is_windows() else 0\n",
    "    print(\"Number of Data Loading Workers:\", num_data_loading_workers)\n",
    "\n",
    "    train_data_loader = DataLoader(\n",
    "        dataset=f_mnist_train, batch_size=wandb.config.batch_size, shuffle=True,\n",
    "        pin_memory=True, num_workers=num_data_loading_workers\n",
    "    )\n",
    "\n",
    "    validation_data_loader = DataLoader(\n",
    "        dataset=f_mnist_validation, batch_size=wandb.config.batch_size,\n",
    "        pin_memory=True, num_workers=num_data_loading_workers\n",
    "    )\n",
    "\n",
    "    return train_data_loader, validation_data_loader\n",
    "\n",
    "# Define CNN model with Dropout\n",
    "class SimpleCNNWithDropout(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        super(SimpleCNNWithDropout, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.dropout = nn.Dropout(dropout_rate)  # Dropout layer with specified rate\n",
    "        self.fc1 = nn.Linear(64 * 14 * 14, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))  # Apply Dropout to the fully connected layer\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def train_model(train_data_loader, validation_data_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Define model\n",
    "    model = SimpleCNNWithDropout(dropout_rate=0.5).to(device)\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=wandb.config.learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Training and Validation loop\n",
    "    for epoch in range(1, wandb.config.epochs + 1):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in train_data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_accuracy = correct / total\n",
    "        train_loss /= len(train_data_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in validation_data_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_accuracy = val_correct / val_total\n",
    "        val_loss /= len(validation_data_loader)\n",
    "\n",
    "        # Log metrics to wandb\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_accuracy\": train_accuracy,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_accuracy\": val_accuracy\n",
    "        })\n",
    "\n",
    "        print(f\"Epoch [{epoch}/{wandb.config.epochs}], Train Loss: {train_loss:.4f}, \"\n",
    "              f\"Train Accuracy: {train_accuracy:.4f}, Validation Loss: {val_loss:.4f}, \"\n",
    "              f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize WandB\n",
    "    wandb.init(\n",
    "        project=\"FashionMNIST_Dropout\",\n",
    "        config={\n",
    "            \"learning_rate\": 0.001,\n",
    "            \"batch_size\": 128,\n",
    "            \"epochs\": 20,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Get Data Loaders\n",
    "    train_data_loader, validation_data_loader = get_fashion_mnist_data()\n",
    "\n",
    "    # Train Model\n",
    "    train_model(train_data_loader, validation_data_loader)\n",
    "\n",
    "    # Finish WandB run\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d01c045c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/work/DL\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/work/DL/wandb/run-20241121_113059-hidc8xy9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout/runs/hidc8xy9' target=\"_blank\">zesty-water-2</a></strong> to <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout/runs/hidc8xy9' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout/runs/hidc8xy9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Train Samples:  55000\n",
      "Num Validation Samples:  5000\n",
      "Sample Shape:  torch.Size([1, 28, 28])\n",
      "Number of Data Loading Workers: 2\n",
      "Epoch [1/30], Train Loss: 0.5735, Train Accuracy: 0.7941, Validation Loss: 0.3287, Validation Accuracy: 0.8770\n",
      "Epoch [2/30], Train Loss: 0.3663, Train Accuracy: 0.8685, Validation Loss: 0.2841, Validation Accuracy: 0.8912\n",
      "Epoch [3/30], Train Loss: 0.3118, Train Accuracy: 0.8878, Validation Loss: 0.2553, Validation Accuracy: 0.9028\n",
      "Epoch [4/30], Train Loss: 0.2781, Train Accuracy: 0.8995, Validation Loss: 0.2401, Validation Accuracy: 0.9070\n",
      "Epoch [5/30], Train Loss: 0.2485, Train Accuracy: 0.9093, Validation Loss: 0.2286, Validation Accuracy: 0.9136\n",
      "Epoch [6/30], Train Loss: 0.2279, Train Accuracy: 0.9175, Validation Loss: 0.2244, Validation Accuracy: 0.9178\n",
      "Epoch [7/30], Train Loss: 0.2080, Train Accuracy: 0.9224, Validation Loss: 0.2184, Validation Accuracy: 0.9170\n",
      "Epoch [8/30], Train Loss: 0.1908, Train Accuracy: 0.9302, Validation Loss: 0.2187, Validation Accuracy: 0.9186\n",
      "Epoch [9/30], Train Loss: 0.1737, Train Accuracy: 0.9359, Validation Loss: 0.2117, Validation Accuracy: 0.9218\n",
      "Epoch [10/30], Train Loss: 0.1599, Train Accuracy: 0.9396, Validation Loss: 0.2168, Validation Accuracy: 0.9244\n",
      "Epoch [11/30], Train Loss: 0.1485, Train Accuracy: 0.9438, Validation Loss: 0.2191, Validation Accuracy: 0.9224\n",
      "Epoch [12/30], Train Loss: 0.1373, Train Accuracy: 0.9489, Validation Loss: 0.2234, Validation Accuracy: 0.9258\n",
      "Epoch [13/30], Train Loss: 0.1243, Train Accuracy: 0.9528, Validation Loss: 0.2234, Validation Accuracy: 0.9250\n",
      "Epoch [14/30], Train Loss: 0.1158, Train Accuracy: 0.9557, Validation Loss: 0.2314, Validation Accuracy: 0.9264\n",
      "Epoch [15/30], Train Loss: 0.1074, Train Accuracy: 0.9593, Validation Loss: 0.2521, Validation Accuracy: 0.9288\n",
      "Epoch [16/30], Train Loss: 0.0995, Train Accuracy: 0.9614, Validation Loss: 0.2412, Validation Accuracy: 0.9296\n",
      "Epoch [17/30], Train Loss: 0.0918, Train Accuracy: 0.9643, Validation Loss: 0.2435, Validation Accuracy: 0.9264\n",
      "Epoch [18/30], Train Loss: 0.0883, Train Accuracy: 0.9663, Validation Loss: 0.2629, Validation Accuracy: 0.9270\n",
      "Epoch [19/30], Train Loss: 0.0821, Train Accuracy: 0.9685, Validation Loss: 0.2868, Validation Accuracy: 0.9248\n",
      "Epoch [20/30], Train Loss: 0.0734, Train Accuracy: 0.9709, Validation Loss: 0.2757, Validation Accuracy: 0.9276\n",
      "Epoch [21/30], Train Loss: 0.0724, Train Accuracy: 0.9714, Validation Loss: 0.2876, Validation Accuracy: 0.9252\n",
      "Epoch [22/30], Train Loss: 0.0676, Train Accuracy: 0.9735, Validation Loss: 0.2957, Validation Accuracy: 0.9270\n",
      "Epoch [23/30], Train Loss: 0.0642, Train Accuracy: 0.9744, Validation Loss: 0.3156, Validation Accuracy: 0.9278\n",
      "Epoch [24/30], Train Loss: 0.0610, Train Accuracy: 0.9759, Validation Loss: 0.3276, Validation Accuracy: 0.9272\n",
      "Epoch [25/30], Train Loss: 0.0597, Train Accuracy: 0.9763, Validation Loss: 0.3148, Validation Accuracy: 0.9290\n",
      "Epoch [26/30], Train Loss: 0.0569, Train Accuracy: 0.9774, Validation Loss: 0.3099, Validation Accuracy: 0.9272\n",
      "Epoch [27/30], Train Loss: 0.0538, Train Accuracy: 0.9781, Validation Loss: 0.3363, Validation Accuracy: 0.9268\n",
      "Epoch [28/30], Train Loss: 0.0525, Train Accuracy: 0.9794, Validation Loss: 0.3408, Validation Accuracy: 0.9270\n",
      "Epoch [29/30], Train Loss: 0.0507, Train Accuracy: 0.9796, Validation Loss: 0.3446, Validation Accuracy: 0.9242\n",
      "Epoch [30/30], Train Loss: 0.0496, Train Accuracy: 0.9796, Validation Loss: 0.3535, Validation Accuracy: 0.9298\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇████████████</td></tr><tr><td>train_loss</td><td>█▅▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▄▅▆▆▆▇▇▇▇▇▇█████▇█▇███████▇█</td></tr><tr><td>val_loss</td><td>▇▅▃▂▂▂▁▁▁▁▁▂▂▂▃▂▃▄▅▄▅▅▆▇▆▆▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>30</td></tr><tr><td>train_accuracy</td><td>0.9796</td></tr><tr><td>train_loss</td><td>0.04965</td></tr><tr><td>val_accuracy</td><td>0.9298</td></tr><tr><td>val_loss</td><td>0.35352</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">zesty-water-2</strong> at: <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout/runs/hidc8xy9' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout/runs/hidc8xy9</a><br/> View project at: <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241121_113059-hidc8xy9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "# Use the current working directory\n",
    "BASE_PATH = Path().resolve()\n",
    "print(BASE_PATH)\n",
    "\n",
    "import sys\n",
    "sys.path.append(str(BASE_PATH))\n",
    "\n",
    "from utils import get_num_cpu_cores, is_linux, is_windows\n",
    "\n",
    "def get_fashion_mnist_data():\n",
    "    data_path = os.path.join(BASE_PATH, \"_00_data\", \"j_fashion_mnist\")\n",
    "\n",
    "    f_mnist_train = datasets.FashionMNIST(data_path, train=True, download=True, transform=transforms.ToTensor())\n",
    "    f_mnist_train, f_mnist_validation = random_split(f_mnist_train, [55_000, 5_000])\n",
    "\n",
    "    print(\"Num Train Samples: \", len(f_mnist_train))\n",
    "    print(\"Num Validation Samples: \", len(f_mnist_validation))\n",
    "    print(\"Sample Shape: \", f_mnist_train[0][0].shape)  # torch.Size([1, 28, 28])\n",
    "\n",
    "    num_data_loading_workers = get_num_cpu_cores() if is_linux() or is_windows() else 0\n",
    "    print(\"Number of Data Loading Workers:\", num_data_loading_workers)\n",
    "\n",
    "    train_data_loader = DataLoader(\n",
    "        dataset=f_mnist_train, batch_size=wandb.config.batch_size, shuffle=True,\n",
    "        pin_memory=True, num_workers=num_data_loading_workers\n",
    "    )\n",
    "\n",
    "    validation_data_loader = DataLoader(\n",
    "        dataset=f_mnist_validation, batch_size=wandb.config.batch_size,\n",
    "        pin_memory=True, num_workers=num_data_loading_workers\n",
    "    )\n",
    "\n",
    "    return train_data_loader, validation_data_loader\n",
    "\n",
    "# Define CNN model with Dropout\n",
    "class SimpleCNNWithDropout(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        super(SimpleCNNWithDropout, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.dropout = nn.Dropout(dropout_rate)  # Dropout layer with specified rate\n",
    "        self.fc1 = nn.Linear(64 * 14 * 14, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))  # Apply Dropout to the fully connected layer\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def train_model(train_data_loader, validation_data_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Define model\n",
    "    model = SimpleCNNWithDropout(dropout_rate=0.5).to(device)\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=wandb.config.learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Training and Validation loop\n",
    "    for epoch in range(1, wandb.config.epochs + 1):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in train_data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_accuracy = correct / total\n",
    "        train_loss /= len(train_data_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in validation_data_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_accuracy = val_correct / val_total\n",
    "        val_loss /= len(validation_data_loader)\n",
    "\n",
    "        # Log metrics to wandb\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_accuracy\": train_accuracy,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_accuracy\": val_accuracy\n",
    "        })\n",
    "\n",
    "        print(f\"Epoch [{epoch}/{wandb.config.epochs}], Train Loss: {train_loss:.4f}, \"\n",
    "              f\"Train Accuracy: {train_accuracy:.4f}, Validation Loss: {val_loss:.4f}, \"\n",
    "              f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize WandB\n",
    "    wandb.init(\n",
    "        project=\"FashionMNIST_Dropout\",\n",
    "        config={\n",
    "            \"learning_rate\": 0.001,\n",
    "            \"batch_size\": 128,\n",
    "            \"epochs\": 30,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Get Data Loaders\n",
    "    train_data_loader, validation_data_loader = get_fashion_mnist_data()\n",
    "\n",
    "    # Train Model\n",
    "    train_model(train_data_loader, validation_data_loader)\n",
    "\n",
    "    # Finish WandB run\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7881756f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/work/DL\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/work/DL/wandb/run-20241121_113427-jr6s3kb6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout/runs/jr6s3kb6' target=\"_blank\">peach-lion-3</a></strong> to <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout/runs/jr6s3kb6' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout/runs/jr6s3kb6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Train Samples:  55000\n",
      "Num Validation Samples:  5000\n",
      "Sample Shape:  torch.Size([1, 28, 28])\n",
      "Number of Data Loading Workers: 2\n",
      "Epoch [1/30], Train Loss: 0.6108, Train Accuracy: 0.7836, Validation Loss: 0.3397, Validation Accuracy: 0.8792\n",
      "Epoch [2/30], Train Loss: 0.3686, Train Accuracy: 0.8699, Validation Loss: 0.2781, Validation Accuracy: 0.9008\n",
      "Epoch [3/30], Train Loss: 0.3189, Train Accuracy: 0.8854, Validation Loss: 0.2623, Validation Accuracy: 0.9068\n",
      "Epoch [4/30], Train Loss: 0.2858, Train Accuracy: 0.8975, Validation Loss: 0.2479, Validation Accuracy: 0.9108\n",
      "Epoch [5/30], Train Loss: 0.2596, Train Accuracy: 0.9056, Validation Loss: 0.2313, Validation Accuracy: 0.9114\n",
      "Epoch [6/30], Train Loss: 0.2418, Train Accuracy: 0.9118, Validation Loss: 0.2181, Validation Accuracy: 0.9192\n",
      "Epoch [7/30], Train Loss: 0.2238, Train Accuracy: 0.9193, Validation Loss: 0.2233, Validation Accuracy: 0.9208\n",
      "Epoch [8/30], Train Loss: 0.2064, Train Accuracy: 0.9239, Validation Loss: 0.2056, Validation Accuracy: 0.9234\n",
      "Epoch [9/30], Train Loss: 0.1936, Train Accuracy: 0.9279, Validation Loss: 0.1968, Validation Accuracy: 0.9278\n",
      "Epoch [10/30], Train Loss: 0.1820, Train Accuracy: 0.9328, Validation Loss: 0.2088, Validation Accuracy: 0.9244\n",
      "Epoch [11/30], Train Loss: 0.1694, Train Accuracy: 0.9381, Validation Loss: 0.1978, Validation Accuracy: 0.9284\n",
      "Epoch [12/30], Train Loss: 0.1571, Train Accuracy: 0.9415, Validation Loss: 0.1986, Validation Accuracy: 0.9312\n",
      "Epoch [13/30], Train Loss: 0.1489, Train Accuracy: 0.9452, Validation Loss: 0.1992, Validation Accuracy: 0.9312\n",
      "Epoch [14/30], Train Loss: 0.1405, Train Accuracy: 0.9477, Validation Loss: 0.2007, Validation Accuracy: 0.9332\n",
      "Epoch [15/30], Train Loss: 0.1300, Train Accuracy: 0.9516, Validation Loss: 0.2060, Validation Accuracy: 0.9326\n",
      "Epoch [16/30], Train Loss: 0.1186, Train Accuracy: 0.9545, Validation Loss: 0.2000, Validation Accuracy: 0.9340\n",
      "Epoch [17/30], Train Loss: 0.1115, Train Accuracy: 0.9575, Validation Loss: 0.2018, Validation Accuracy: 0.9370\n",
      "Epoch [18/30], Train Loss: 0.1021, Train Accuracy: 0.9606, Validation Loss: 0.2178, Validation Accuracy: 0.9312\n",
      "Epoch [19/30], Train Loss: 0.0986, Train Accuracy: 0.9611, Validation Loss: 0.2221, Validation Accuracy: 0.9342\n",
      "Epoch [20/30], Train Loss: 0.0939, Train Accuracy: 0.9642, Validation Loss: 0.2072, Validation Accuracy: 0.9376\n",
      "Epoch [21/30], Train Loss: 0.0856, Train Accuracy: 0.9678, Validation Loss: 0.2337, Validation Accuracy: 0.9278\n",
      "Epoch [22/30], Train Loss: 0.0814, Train Accuracy: 0.9693, Validation Loss: 0.2255, Validation Accuracy: 0.9348\n",
      "Epoch [23/30], Train Loss: 0.0765, Train Accuracy: 0.9711, Validation Loss: 0.2417, Validation Accuracy: 0.9360\n",
      "Epoch [24/30], Train Loss: 0.0726, Train Accuracy: 0.9718, Validation Loss: 0.2454, Validation Accuracy: 0.9332\n",
      "Epoch [25/30], Train Loss: 0.0707, Train Accuracy: 0.9724, Validation Loss: 0.2533, Validation Accuracy: 0.9348\n",
      "Epoch [26/30], Train Loss: 0.0690, Train Accuracy: 0.9731, Validation Loss: 0.2536, Validation Accuracy: 0.9338\n",
      "Epoch [27/30], Train Loss: 0.0630, Train Accuracy: 0.9753, Validation Loss: 0.2582, Validation Accuracy: 0.9366\n",
      "Epoch [28/30], Train Loss: 0.0583, Train Accuracy: 0.9775, Validation Loss: 0.2848, Validation Accuracy: 0.9334\n",
      "Epoch [29/30], Train Loss: 0.0569, Train Accuracy: 0.9785, Validation Loss: 0.2666, Validation Accuracy: 0.9350\n",
      "Epoch [30/30], Train Loss: 0.0551, Train Accuracy: 0.9787, Validation Loss: 0.2724, Validation Accuracy: 0.9330\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇██████████</td></tr><tr><td>train_loss</td><td>█▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▄▅▅▆▆▆▇▆▇▇▇▇▇██▇██▇██▇███▇█▇</td></tr><tr><td>val_loss</td><td>█▅▄▄▃▂▂▁▁▂▁▁▁▁▁▁▁▂▂▂▃▂▃▃▄▄▄▅▄▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>30</td></tr><tr><td>train_accuracy</td><td>0.97869</td></tr><tr><td>train_loss</td><td>0.05514</td></tr><tr><td>val_accuracy</td><td>0.933</td></tr><tr><td>val_loss</td><td>0.27241</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">peach-lion-3</strong> at: <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout/runs/jr6s3kb6' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout/runs/jr6s3kb6</a><br/> View project at: <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241121_113427-jr6s3kb6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "# Use the current working directory\n",
    "BASE_PATH = Path().resolve()\n",
    "print(BASE_PATH)\n",
    "\n",
    "import sys\n",
    "sys.path.append(str(BASE_PATH))\n",
    "\n",
    "from utils import get_num_cpu_cores, is_linux, is_windows\n",
    "\n",
    "def get_fashion_mnist_data():\n",
    "    data_path = os.path.join(BASE_PATH, \"_00_data\", \"j_fashion_mnist\")\n",
    "\n",
    "    f_mnist_train = datasets.FashionMNIST(data_path, train=True, download=True, transform=transforms.ToTensor())\n",
    "    f_mnist_train, f_mnist_validation = random_split(f_mnist_train, [55_000, 5_000])\n",
    "\n",
    "    print(\"Num Train Samples: \", len(f_mnist_train))\n",
    "    print(\"Num Validation Samples: \", len(f_mnist_validation))\n",
    "    print(\"Sample Shape: \", f_mnist_train[0][0].shape)  # torch.Size([1, 28, 28])\n",
    "\n",
    "    num_data_loading_workers = get_num_cpu_cores() if is_linux() or is_windows() else 0\n",
    "    print(\"Number of Data Loading Workers:\", num_data_loading_workers)\n",
    "\n",
    "    train_data_loader = DataLoader(\n",
    "        dataset=f_mnist_train, batch_size=wandb.config.batch_size, shuffle=True,\n",
    "        pin_memory=True, num_workers=num_data_loading_workers\n",
    "    )\n",
    "\n",
    "    validation_data_loader = DataLoader(\n",
    "        dataset=f_mnist_validation, batch_size=wandb.config.batch_size,\n",
    "        pin_memory=True, num_workers=num_data_loading_workers\n",
    "    )\n",
    "\n",
    "    return train_data_loader, validation_data_loader\n",
    "\n",
    "# Define CNN model with Dropout\n",
    "class SimpleCNNWithDropout(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        super(SimpleCNNWithDropout, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.dropout = nn.Dropout(dropout_rate)  # Dropout layer with specified rate\n",
    "        self.fc1 = nn.Linear(64 * 14 * 14, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))  # Apply Dropout to the fully connected layer\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def train_model(train_data_loader, validation_data_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Define model\n",
    "    model = SimpleCNNWithDropout(dropout_rate=0.5).to(device)\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=wandb.config.learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Training and Validation loop\n",
    "    for epoch in range(1, wandb.config.epochs + 1):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in train_data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_accuracy = correct / total\n",
    "        train_loss /= len(train_data_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in validation_data_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_accuracy = val_correct / val_total\n",
    "        val_loss /= len(validation_data_loader)\n",
    "\n",
    "        # Log metrics to wandb\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_accuracy\": train_accuracy,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_accuracy\": val_accuracy\n",
    "        })\n",
    "\n",
    "        print(f\"Epoch [{epoch}/{wandb.config.epochs}], Train Loss: {train_loss:.4f}, \"\n",
    "              f\"Train Accuracy: {train_accuracy:.4f}, Validation Loss: {val_loss:.4f}, \"\n",
    "              f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize WandB\n",
    "    wandb.init(\n",
    "        project=\"FashionMNIST_Dropout\",\n",
    "        config={\n",
    "            \"learning_rate\": 0.001,\n",
    "            \"batch_size\": 256,\n",
    "            \"epochs\": 30,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Get Data Loaders\n",
    "    train_data_loader, validation_data_loader = get_fashion_mnist_data()\n",
    "\n",
    "    # Train Model\n",
    "    train_model(train_data_loader, validation_data_loader)\n",
    "\n",
    "    # Finish WandB run\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddc50c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8d11f2dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/work/DL\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/work/DL/wandb/run-20241121_113648-gmbvxwu9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout/runs/gmbvxwu9' target=\"_blank\">efficient-oath-4</a></strong> to <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout/runs/gmbvxwu9' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout/runs/gmbvxwu9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Train Samples:  55000\n",
      "Num Validation Samples:  5000\n",
      "Sample Shape:  torch.Size([1, 28, 28])\n",
      "Number of Data Loading Workers: 2\n",
      "Epoch [1/30], Train Loss: 0.7236, Train Accuracy: 0.7424, Validation Loss: 0.3809, Validation Accuracy: 0.8602\n",
      "Epoch [2/30], Train Loss: 0.4043, Train Accuracy: 0.8573, Validation Loss: 0.3302, Validation Accuracy: 0.8760\n",
      "Epoch [3/30], Train Loss: 0.3451, Train Accuracy: 0.8767, Validation Loss: 0.3008, Validation Accuracy: 0.8894\n",
      "Epoch [4/30], Train Loss: 0.3099, Train Accuracy: 0.8898, Validation Loss: 0.2734, Validation Accuracy: 0.9002\n",
      "Epoch [5/30], Train Loss: 0.2843, Train Accuracy: 0.8981, Validation Loss: 0.2659, Validation Accuracy: 0.9038\n",
      "Epoch [6/30], Train Loss: 0.2666, Train Accuracy: 0.9036, Validation Loss: 0.2523, Validation Accuracy: 0.9064\n",
      "Epoch [7/30], Train Loss: 0.2479, Train Accuracy: 0.9112, Validation Loss: 0.2380, Validation Accuracy: 0.9116\n",
      "Epoch [8/30], Train Loss: 0.2325, Train Accuracy: 0.9165, Validation Loss: 0.2394, Validation Accuracy: 0.9136\n",
      "Epoch [9/30], Train Loss: 0.2202, Train Accuracy: 0.9203, Validation Loss: 0.2327, Validation Accuracy: 0.9192\n",
      "Epoch [10/30], Train Loss: 0.2085, Train Accuracy: 0.9238, Validation Loss: 0.2383, Validation Accuracy: 0.9160\n",
      "Epoch [11/30], Train Loss: 0.1956, Train Accuracy: 0.9281, Validation Loss: 0.2392, Validation Accuracy: 0.9184\n",
      "Epoch [12/30], Train Loss: 0.1854, Train Accuracy: 0.9307, Validation Loss: 0.2288, Validation Accuracy: 0.9194\n",
      "Epoch [13/30], Train Loss: 0.1755, Train Accuracy: 0.9364, Validation Loss: 0.2395, Validation Accuracy: 0.9166\n",
      "Epoch [14/30], Train Loss: 0.1685, Train Accuracy: 0.9375, Validation Loss: 0.2231, Validation Accuracy: 0.9218\n",
      "Epoch [15/30], Train Loss: 0.1559, Train Accuracy: 0.9421, Validation Loss: 0.2323, Validation Accuracy: 0.9204\n",
      "Epoch [16/30], Train Loss: 0.1478, Train Accuracy: 0.9455, Validation Loss: 0.2217, Validation Accuracy: 0.9246\n",
      "Epoch [17/30], Train Loss: 0.1384, Train Accuracy: 0.9491, Validation Loss: 0.2260, Validation Accuracy: 0.9236\n",
      "Epoch [18/30], Train Loss: 0.1333, Train Accuracy: 0.9497, Validation Loss: 0.2276, Validation Accuracy: 0.9234\n",
      "Epoch [19/30], Train Loss: 0.1253, Train Accuracy: 0.9521, Validation Loss: 0.2328, Validation Accuracy: 0.9234\n",
      "Epoch [20/30], Train Loss: 0.1180, Train Accuracy: 0.9548, Validation Loss: 0.2394, Validation Accuracy: 0.9206\n",
      "Epoch [21/30], Train Loss: 0.1120, Train Accuracy: 0.9572, Validation Loss: 0.2408, Validation Accuracy: 0.9260\n",
      "Epoch [22/30], Train Loss: 0.1068, Train Accuracy: 0.9584, Validation Loss: 0.2426, Validation Accuracy: 0.9242\n",
      "Epoch [23/30], Train Loss: 0.1011, Train Accuracy: 0.9615, Validation Loss: 0.2508, Validation Accuracy: 0.9228\n",
      "Epoch [24/30], Train Loss: 0.0965, Train Accuracy: 0.9631, Validation Loss: 0.2502, Validation Accuracy: 0.9236\n",
      "Epoch [25/30], Train Loss: 0.0924, Train Accuracy: 0.9649, Validation Loss: 0.2626, Validation Accuracy: 0.9252\n",
      "Epoch [26/30], Train Loss: 0.0861, Train Accuracy: 0.9660, Validation Loss: 0.2735, Validation Accuracy: 0.9264\n",
      "Epoch [27/30], Train Loss: 0.0834, Train Accuracy: 0.9683, Validation Loss: 0.2901, Validation Accuracy: 0.9256\n",
      "Epoch [28/30], Train Loss: 0.0788, Train Accuracy: 0.9699, Validation Loss: 0.2642, Validation Accuracy: 0.9286\n",
      "Epoch [29/30], Train Loss: 0.0757, Train Accuracy: 0.9709, Validation Loss: 0.2943, Validation Accuracy: 0.9266\n",
      "Epoch [30/30], Train Loss: 0.0710, Train Accuracy: 0.9718, Validation Loss: 0.2912, Validation Accuracy: 0.9236\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇██████████</td></tr><tr><td>train_loss</td><td>█▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▄▅▅▆▆▆▇▇▇▇▇▇▇█▇▇▇▇██▇▇█████▇</td></tr><tr><td>val_loss</td><td>█▆▄▃▃▂▂▂▁▂▂▁▂▁▁▁▁▁▁▂▂▂▂▂▃▃▄▃▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>30</td></tr><tr><td>train_accuracy</td><td>0.9718</td></tr><tr><td>train_loss</td><td>0.07104</td></tr><tr><td>val_accuracy</td><td>0.9236</td></tr><tr><td>val_loss</td><td>0.29122</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">efficient-oath-4</strong> at: <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout/runs/gmbvxwu9' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout/runs/gmbvxwu9</a><br/> View project at: <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241121_113648-gmbvxwu9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Define CNN model with Dropout\n",
    "class SimpleCNNWithDropout(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        super(SimpleCNNWithDropout, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.dropout = nn.Dropout(dropout_rate)  # Dropout layer with specified rate\n",
    "        self.fc1 = nn.Linear(64 * 14 * 14, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))  # Apply Dropout to the fully connected layer\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize WandB\n",
    "    wandb.init(\n",
    "        project=\"FashionMNIST_Dropout\",\n",
    "        config={\n",
    "            \"learning_rate\": 0.001,\n",
    "            \"batch_size\": 512,\n",
    "            \"epochs\": 30,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a52b9659",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/work/DL\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/work/DL/wandb/run-20241121_113948-mzus9lt4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout/runs/mzus9lt4' target=\"_blank\">lunar-pond-5</a></strong> to <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout/runs/mzus9lt4' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout/runs/mzus9lt4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Train Samples:  55000\n",
      "Num Validation Samples:  5000\n",
      "Sample Shape:  torch.Size([1, 28, 28])\n",
      "Number of Data Loading Workers: 2\n",
      "Epoch [1/20], Train Loss: 0.5382, Train Accuracy: 0.8079, Validation Loss: 0.2914, Validation Accuracy: 0.8940\n",
      "Epoch [2/20], Train Loss: 0.3315, Train Accuracy: 0.8818, Validation Loss: 0.2612, Validation Accuracy: 0.9074\n",
      "Epoch [3/20], Train Loss: 0.2836, Train Accuracy: 0.8972, Validation Loss: 0.2221, Validation Accuracy: 0.9170\n",
      "Epoch [4/20], Train Loss: 0.2487, Train Accuracy: 0.9088, Validation Loss: 0.2212, Validation Accuracy: 0.9238\n",
      "Epoch [5/20], Train Loss: 0.2281, Train Accuracy: 0.9160, Validation Loss: 0.2111, Validation Accuracy: 0.9236\n",
      "Epoch [6/20], Train Loss: 0.2044, Train Accuracy: 0.9246, Validation Loss: 0.2146, Validation Accuracy: 0.9270\n",
      "Epoch [7/20], Train Loss: 0.1959, Train Accuracy: 0.9270, Validation Loss: 0.2201, Validation Accuracy: 0.9202\n",
      "Epoch [8/20], Train Loss: 0.1808, Train Accuracy: 0.9325, Validation Loss: 0.2125, Validation Accuracy: 0.9284\n",
      "Epoch [9/20], Train Loss: 0.1681, Train Accuracy: 0.9352, Validation Loss: 0.2203, Validation Accuracy: 0.9254\n",
      "Epoch [10/20], Train Loss: 0.1585, Train Accuracy: 0.9391, Validation Loss: 0.2276, Validation Accuracy: 0.9260\n",
      "Epoch [11/20], Train Loss: 0.1491, Train Accuracy: 0.9435, Validation Loss: 0.2322, Validation Accuracy: 0.9260\n",
      "Epoch [12/20], Train Loss: 0.1420, Train Accuracy: 0.9462, Validation Loss: 0.2470, Validation Accuracy: 0.9254\n",
      "Epoch [13/20], Train Loss: 0.1342, Train Accuracy: 0.9490, Validation Loss: 0.2553, Validation Accuracy: 0.9262\n",
      "Epoch [14/20], Train Loss: 0.1262, Train Accuracy: 0.9513, Validation Loss: 0.2492, Validation Accuracy: 0.9264\n",
      "Epoch [15/20], Train Loss: 0.1208, Train Accuracy: 0.9530, Validation Loss: 0.2690, Validation Accuracy: 0.9254\n",
      "Epoch [16/20], Train Loss: 0.1158, Train Accuracy: 0.9555, Validation Loss: 0.2757, Validation Accuracy: 0.9274\n",
      "Epoch [17/20], Train Loss: 0.1097, Train Accuracy: 0.9579, Validation Loss: 0.2854, Validation Accuracy: 0.9306\n",
      "Epoch [18/20], Train Loss: 0.1087, Train Accuracy: 0.9583, Validation Loss: 0.3032, Validation Accuracy: 0.9246\n",
      "Epoch [19/20], Train Loss: 0.1029, Train Accuracy: 0.9596, Validation Loss: 0.3079, Validation Accuracy: 0.9236\n",
      "Epoch [20/20], Train Loss: 0.1028, Train Accuracy: 0.9604, Validation Loss: 0.3011, Validation Accuracy: 0.9240\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>train_accuracy</td><td>▁▄▅▆▆▆▆▇▇▇▇▇▇███████</td></tr><tr><td>train_loss</td><td>█▅▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▅▇▇▇▆█▇▇▇▇▇▇▇▇█▇▇▇</td></tr><tr><td>val_loss</td><td>▇▅▂▂▁▁▂▁▂▂▃▄▄▄▅▆▆███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>train_accuracy</td><td>0.96042</td></tr><tr><td>train_loss</td><td>0.10285</td></tr><tr><td>val_accuracy</td><td>0.924</td></tr><tr><td>val_loss</td><td>0.30106</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lunar-pond-5</strong> at: <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout/runs/mzus9lt4' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout/runs/mzus9lt4</a><br/> View project at: <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241121_113948-mzus9lt4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Define CNN model with Dropout\n",
    "class SimpleCNNWithDropout(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        super(SimpleCNNWithDropout, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.dropout = nn.Dropout(dropout_rate)  # Dropout layer with specified rate\n",
    "        self.fc1 = nn.Linear(64 * 14 * 14, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))  # Apply Dropout to the fully connected layer\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize WandB\n",
    "    wandb.init(\n",
    "        project=\"FashionMNIST_Dropout\",\n",
    "        config={\n",
    "            \"learning_rate\": 0.005,\n",
    "            \"batch_size\": 256,\n",
    "            \"epochs\": 20,\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "459cd041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/work/DL\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/work/DL/wandb/run-20241121_114315-hmn4gnz7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout_Normalized/runs/hmn4gnz7' target=\"_blank\">effortless-dream-1</a></strong> to <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout_Normalized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout_Normalized' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout_Normalized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout_Normalized/runs/hmn4gnz7' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout_Normalized/runs/hmn4gnz7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Train Samples:  55000\n",
      "Num Validation Samples:  5000\n",
      "Sample Shape:  torch.Size([1, 28, 28])\n",
      "Number of Data Loading Workers: 2\n",
      "Epoch [1/30], Train Loss: 0.6278, Train Accuracy: 0.7806, Validation Loss: 0.3548, Validation Accuracy: 0.8696\n",
      "Epoch [2/30], Train Loss: 0.3844, Train Accuracy: 0.8633, Validation Loss: 0.2936, Validation Accuracy: 0.8906\n",
      "Epoch [3/30], Train Loss: 0.3279, Train Accuracy: 0.8828, Validation Loss: 0.2797, Validation Accuracy: 0.8932\n",
      "Epoch [4/30], Train Loss: 0.2877, Train Accuracy: 0.8957, Validation Loss: 0.2436, Validation Accuracy: 0.9100\n",
      "Epoch [5/30], Train Loss: 0.2609, Train Accuracy: 0.9056, Validation Loss: 0.2418, Validation Accuracy: 0.9090\n",
      "Epoch [6/30], Train Loss: 0.2368, Train Accuracy: 0.9142, Validation Loss: 0.2288, Validation Accuracy: 0.9178\n",
      "Epoch [7/30], Train Loss: 0.2175, Train Accuracy: 0.9198, Validation Loss: 0.2269, Validation Accuracy: 0.9142\n",
      "Epoch [8/30], Train Loss: 0.2020, Train Accuracy: 0.9262, Validation Loss: 0.2190, Validation Accuracy: 0.9202\n",
      "Epoch [9/30], Train Loss: 0.1877, Train Accuracy: 0.9299, Validation Loss: 0.2213, Validation Accuracy: 0.9250\n",
      "Epoch [10/30], Train Loss: 0.1740, Train Accuracy: 0.9356, Validation Loss: 0.2223, Validation Accuracy: 0.9246\n",
      "Epoch [11/30], Train Loss: 0.1620, Train Accuracy: 0.9402, Validation Loss: 0.2279, Validation Accuracy: 0.9208\n",
      "Epoch [12/30], Train Loss: 0.1486, Train Accuracy: 0.9439, Validation Loss: 0.2180, Validation Accuracy: 0.9256\n",
      "Epoch [13/30], Train Loss: 0.1396, Train Accuracy: 0.9477, Validation Loss: 0.2328, Validation Accuracy: 0.9256\n",
      "Epoch [14/30], Train Loss: 0.1288, Train Accuracy: 0.9509, Validation Loss: 0.2319, Validation Accuracy: 0.9288\n",
      "Epoch [15/30], Train Loss: 0.1166, Train Accuracy: 0.9560, Validation Loss: 0.2329, Validation Accuracy: 0.9268\n",
      "Epoch [16/30], Train Loss: 0.1102, Train Accuracy: 0.9579, Validation Loss: 0.2451, Validation Accuracy: 0.9256\n",
      "Epoch [17/30], Train Loss: 0.1062, Train Accuracy: 0.9591, Validation Loss: 0.2510, Validation Accuracy: 0.9284\n",
      "Epoch [18/30], Train Loss: 0.0978, Train Accuracy: 0.9613, Validation Loss: 0.2605, Validation Accuracy: 0.9270\n",
      "Epoch [19/30], Train Loss: 0.0923, Train Accuracy: 0.9640, Validation Loss: 0.2578, Validation Accuracy: 0.9280\n",
      "Epoch [20/30], Train Loss: 0.0856, Train Accuracy: 0.9661, Validation Loss: 0.2662, Validation Accuracy: 0.9302\n",
      "Epoch [21/30], Train Loss: 0.0819, Train Accuracy: 0.9685, Validation Loss: 0.2817, Validation Accuracy: 0.9248\n",
      "Epoch [22/30], Train Loss: 0.0781, Train Accuracy: 0.9688, Validation Loss: 0.2895, Validation Accuracy: 0.9298\n",
      "Epoch [23/30], Train Loss: 0.0747, Train Accuracy: 0.9707, Validation Loss: 0.2998, Validation Accuracy: 0.9284\n",
      "Epoch [24/30], Train Loss: 0.0698, Train Accuracy: 0.9723, Validation Loss: 0.3167, Validation Accuracy: 0.9268\n",
      "Epoch [25/30], Train Loss: 0.0662, Train Accuracy: 0.9733, Validation Loss: 0.3110, Validation Accuracy: 0.9262\n",
      "Epoch [26/30], Train Loss: 0.0640, Train Accuracy: 0.9743, Validation Loss: 0.3374, Validation Accuracy: 0.9272\n",
      "Epoch [27/30], Train Loss: 0.0611, Train Accuracy: 0.9753, Validation Loss: 0.3239, Validation Accuracy: 0.9286\n",
      "Epoch [28/30], Train Loss: 0.0569, Train Accuracy: 0.9772, Validation Loss: 0.3387, Validation Accuracy: 0.9286\n",
      "Epoch [29/30], Train Loss: 0.0606, Train Accuracy: 0.9762, Validation Loss: 0.3299, Validation Accuracy: 0.9282\n",
      "Epoch [30/30], Train Loss: 0.0557, Train Accuracy: 0.9781, Validation Loss: 0.3365, Validation Accuracy: 0.9288\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇████████████</td></tr><tr><td>train_loss</td><td>█▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▄▆▆▇▆▇▇▇▇▇▇██▇████▇█████████</td></tr><tr><td>val_loss</td><td>█▅▄▂▂▂▁▁▁▁▂▁▂▂▂▂▃▃▃▃▄▅▅▆▆▇▆▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>30</td></tr><tr><td>train_accuracy</td><td>0.97811</td></tr><tr><td>train_loss</td><td>0.05568</td></tr><tr><td>val_accuracy</td><td>0.9288</td></tr><tr><td>val_loss</td><td>0.33649</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">effortless-dream-1</strong> at: <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout_Normalized/runs/hmn4gnz7' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout_Normalized/runs/hmn4gnz7</a><br/> View project at: <a href='https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout_Normalized' target=\"_blank\">https://wandb.ai/-ddj127-korea-university-of-technology-and-education/FashionMNIST_Dropout_Normalized</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241121_114315-hmn4gnz7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "# Use the current working directory\n",
    "BASE_PATH = Path().resolve()\n",
    "print(BASE_PATH)\n",
    "\n",
    "import sys\n",
    "sys.path.append(str(BASE_PATH))\n",
    "\n",
    "from utils import get_num_cpu_cores, is_linux, is_windows\n",
    "\n",
    "def get_fashion_mnist_data():\n",
    "    data_path = os.path.join(BASE_PATH, \"_00_data\", \"j_fashion_mnist\")\n",
    "\n",
    "    # Add Normalize to the transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))  # Normalize with mean=0.5 and std=0.5\n",
    "    ])\n",
    "\n",
    "    f_mnist_train = datasets.FashionMNIST(data_path, train=True, download=True, transform=transform)\n",
    "    f_mnist_train, f_mnist_validation = random_split(f_mnist_train, [55_000, 5_000])\n",
    "\n",
    "    print(\"Num Train Samples: \", len(f_mnist_train))\n",
    "    print(\"Num Validation Samples: \", len(f_mnist_validation))\n",
    "    print(\"Sample Shape: \", f_mnist_train[0][0].shape)  # torch.Size([1, 28, 28])\n",
    "\n",
    "    num_data_loading_workers = get_num_cpu_cores() if is_linux() or is_windows() else 0\n",
    "    print(\"Number of Data Loading Workers:\", num_data_loading_workers)\n",
    "\n",
    "    train_data_loader = DataLoader(\n",
    "        dataset=f_mnist_train, batch_size=wandb.config.batch_size, shuffle=True,\n",
    "        pin_memory=True, num_workers=num_data_loading_workers\n",
    "    )\n",
    "\n",
    "    validation_data_loader = DataLoader(\n",
    "        dataset=f_mnist_validation, batch_size=wandb.config.batch_size,\n",
    "        pin_memory=True, num_workers=num_data_loading_workers\n",
    "    )\n",
    "\n",
    "    return train_data_loader, validation_data_loader\n",
    "\n",
    "# Define CNN model with Dropout\n",
    "class SimpleCNNWithDropout(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        super(SimpleCNNWithDropout, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.dropout = nn.Dropout(dropout_rate)  # Dropout layer with specified rate\n",
    "        self.fc1 = nn.Linear(64 * 14 * 14, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))  # Apply Dropout to the fully connected layer\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def train_model(train_data_loader, validation_data_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Define model\n",
    "    model = SimpleCNNWithDropout(dropout_rate=0.5).to(device)\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=wandb.config.learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Training and Validation loop\n",
    "    for epoch in range(1, wandb.config.epochs + 1):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in train_data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_accuracy = correct / total\n",
    "        train_loss /= len(train_data_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in validation_data_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_accuracy = val_correct / val_total\n",
    "        val_loss /= len(validation_data_loader)\n",
    "\n",
    "        # Log metrics to wandb\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_accuracy\": train_accuracy,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_accuracy\": val_accuracy\n",
    "        })\n",
    "\n",
    "        print(f\"Epoch [{epoch}/{wandb.config.epochs}], Train Loss: {train_loss:.4f}, \"\n",
    "              f\"Train Accuracy: {train_accuracy:.4f}, Validation Loss: {val_loss:.4f}, \"\n",
    "              f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize WandB\n",
    "    wandb.init(\n",
    "        project=\"FashionMNIST_Dropout_Normalized\",\n",
    "        config={\n",
    "            \"learning_rate\": 0.001,\n",
    "            \"batch_size\": 256,\n",
    "            \"epochs\": 30,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Get Data Loaders\n",
    "    train_data_loader, validation_data_loader = get_fashion_mnist_data()\n",
    "\n",
    "    # Train Model\n",
    "    train_model(train_data_loader, validation_data_loader)\n",
    "\n",
    "    # Finish WandB run\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6826dba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "# Use the current working directory\n",
    "BASE_PATH = Path().resolve()\n",
    "print(BASE_PATH)\n",
    "\n",
    "import sys\n",
    "sys.path.append(str(BASE_PATH))\n",
    "\n",
    "from utils import get_num_cpu_cores, is_linux, is_windows\n",
    "\n",
    "def get_fashion_mnist_data():\n",
    "    data_path = os.path.join(BASE_PATH, \"_00_data\", \"j_fashion_mnist\")\n",
    "\n",
    "    # Add Normalize to the transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))  # Normalize with mean=0.5 and std=0.5\n",
    "    ])\n",
    "\n",
    "    f_mnist_train = datasets.FashionMNIST(data_path, train=True, download=True, transform=transform)\n",
    "    f_mnist_train, f_mnist_validation = random_split(f_mnist_train, [55_000, 5_000])\n",
    "\n",
    "    print(\"Num Train Samples: \", len(f_mnist_train))\n",
    "    print(\"Num Validation Samples: \", len(f_mnist_validation))\n",
    "    print(\"Sample Shape: \", f_mnist_train[0][0].shape)  # torch.Size([1, 28, 28])\n",
    "\n",
    "    num_data_loading_workers = get_num_cpu_cores() if is_linux() or is_windows() else 0\n",
    "    print(\"Number of Data Loading Workers:\", num_data_loading_workers)\n",
    "\n",
    "    train_data_loader = DataLoader(\n",
    "        dataset=f_mnist_train, batch_size=wandb.config.batch_size, shuffle=True,\n",
    "        pin_memory=True, num_workers=num_data_loading_workers\n",
    "    )\n",
    "\n",
    "    validation_data_loader = DataLoader(\n",
    "        dataset=f_mnist_validation, batch_size=wandb.config.batch_size,\n",
    "        pin_memory=True, num_workers=num_data_loading_workers\n",
    "    )\n",
    "\n",
    "    return train_data_loader, validation_data_loader\n",
    "\n",
    "# Define CNN model with Dropout\n",
    "class SimpleCNNWithDropout(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        super(SimpleCNNWithDropout, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.dropout = nn.Dropout(dropout_rate)  # Dropout layer with specified rate\n",
    "        self.fc1 = nn.Linear(64 * 14 * 14, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))  # Apply Dropout to the fully connected layer\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def train_model(train_data_loader, validation_data_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Define model\n",
    "    model = SimpleCNNWithDropout(dropout_rate=0.5).to(device)\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=wandb.config.learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Training and Validation loop\n",
    "    for epoch in range(1, wandb.config.epochs + 1):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in train_data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_accuracy = correct / total\n",
    "        train_loss /= len(train_data_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in validation_data_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_accuracy = val_correct / val_total\n",
    "        val_loss /= len(validation_data_loader)\n",
    "\n",
    "        # Log metrics to wandb\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_accuracy\": train_accuracy,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_accuracy\": val_accuracy\n",
    "        })\n",
    "\n",
    "        print(f\"Epoch [{epoch}/{wandb.config.epochs}], Train Loss: {train_loss:.4f}, \"\n",
    "              f\"Train Accuracy: {train_accuracy:.4f}, Validation Loss: {val_loss:.4f}, \"\n",
    "              f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize WandB\n",
    "    wandb.init(\n",
    "        project=\"FashionMNIST_Dropout_Normalized\",\n",
    "        config={\n",
    "            \"learning_rate\": 0.001,\n",
    "            \"batch_size\": 128,\n",
    "            \"epochs\": 30,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Get Data Loaders\n",
    "    train_data_loader, validation_data_loader = get_fashion_mnist_data()\n",
    "\n",
    "    # Train Model\n",
    "    train_model(train_data_loader, validation_data_loader)\n",
    "\n",
    "    # Finish WandB run\n",
    "    wandb.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.3 (NGC 24.03/Python 3.10) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
